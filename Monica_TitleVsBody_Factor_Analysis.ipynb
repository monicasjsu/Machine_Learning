{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Monica_TitleVsBody_Factor_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monicasjsu/Machine_Learning/blob/main/Monica_TitleVsBody_Factor_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cURC77eCYB97"
      },
      "source": [
        "#**Team Name : Shecodes**\n",
        "\n",
        "GitHub URL: \n",
        "\n",
        "### **Team Contributions:**\n",
        "\n",
        "|Features  |  Member |\n",
        "|-----|-----|\n",
        "| Title vs BOdy                         |  Monica Dommaraju |  \n",
        "| Content Statistics                  |  Sri Sruthi Chilukuri  |  \n",
        "|   Writting Style                           |  Swati Narkhede \n",
        "| Misleading Intents                 |  Asha Aher |  \n",
        "\n",
        "\n",
        "### **Enrichment Dataset Details:**\n",
        "\n",
        "- Kaggle Fakenews Dataset\n",
        "- Liar Liar Dataset\n",
        "- Politifact Fake news and Real News Content\n",
        "- Stance Dataset from Fake News Challenge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KELyb45uny3"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPbxJKXZrhgP",
        "outputId": "003ffe2a-a0e5-40d0-aec1-5f75645cb4d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import gensim\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn import metrics\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "from gensim.models.doc2vec import TaggedDocument\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WaBgPVmu0Su"
      },
      "source": [
        "# Load the Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMRGX7Nxu4vA"
      },
      "source": [
        "## Kaggle Fake News Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An07unApxzX_",
        "outputId": "cd0559f1-f265-4140-9514-8a18a80585df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y6nxTktrkTx",
        "outputId": "63c46dd8-e134-4a72-8f9c-8a5b133bb25d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "fake_train = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/fakenews/train_fakenews.csv\",sep=',')\n",
        "fake_test = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/fakenews/test_fakenews.csv\",sep=',')\n",
        "fake_submit = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/fakenews/submit_fakenews.csv\",sep=',')\n",
        "fake_test = pd.merge(fake_test, fake_submit, on=\"id\", how='left')\n",
        "fake_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... label\n",
              "0   0  ...     1\n",
              "1   1  ...     0\n",
              "2   2  ...     1\n",
              "3   3  ...     1\n",
              "4   4  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW6-lJxNs7bk",
        "outputId": "7269f24d-bfac-4735-ced6-668ffe34b7b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "fake_train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20800 entries, 0 to 20799\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      20800 non-null  int64 \n",
            " 1   title   20242 non-null  object\n",
            " 2   author  18843 non-null  object\n",
            " 3   text    20761 non-null  object\n",
            " 4   label   20800 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 812.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RhUAdHis9Zo",
        "outputId": "4dc5093b-c55c-4248-a65a-a08ef4d8d9c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "fake_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20800</td>\n",
              "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
              "      <td>David Streitfeld</td>\n",
              "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20801</td>\n",
              "      <td>Russian warships ready to strike terrorists ne...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Russian warships ready to strike terrorists ne...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20802</td>\n",
              "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
              "      <td>Common Dreams</td>\n",
              "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20803</td>\n",
              "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
              "      <td>Daniel Victor</td>\n",
              "      <td>If at first you don’t succeed, try a different...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20804</td>\n",
              "      <td>Keiser Report: Meme Wars (E995)</td>\n",
              "      <td>Truth Broadcast Network</td>\n",
              "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ... label\n",
              "0  20800  ...     0\n",
              "1  20801  ...     1\n",
              "2  20802  ...     0\n",
              "3  20803  ...     1\n",
              "4  20804  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ3-lp0As_gG",
        "outputId": "e91c8edb-6dfe-42f1-c702-8e358946285b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "fake_test.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5200 entries, 0 to 5199\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      5200 non-null   int64 \n",
            " 1   title   5078 non-null   object\n",
            " 2   author  4697 non-null   object\n",
            " 3   text    5193 non-null   object\n",
            " 4   label   5200 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 243.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3rIZd2kviKj"
      },
      "source": [
        "## Liar Liar Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVb6lT2ZvlIZ"
      },
      "source": [
        "# Load Liar-Liar dataset\n",
        "columns = [\n",
        "  'jsonid', \n",
        "  'label', \n",
        "  'text', \n",
        "  'subject', \n",
        "  'speaker', \n",
        "  'speakerjobtitle', \n",
        "  'stateinfo',\n",
        "  'partyaffiliation', \n",
        "  'barelytruecounts', \n",
        "  'falsecounts',\n",
        "  'halftruecounts',\n",
        "  'mostlytrueocunts',\n",
        "  'pantsonfirecounts',\n",
        "  'context'\n",
        "  ]\n",
        "train_news = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/liar_dataset/train.tsv\",sep='\\t', names=columns)\n",
        "test_news = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/liar_dataset/test.tsv\",sep='\\t', names=columns)\n",
        "valid_news = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/liar_dataset/valid.tsv\",sep='\\t', names=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH1SCxC0vrI9",
        "outputId": "ca20c723-3fe3-4a20-82d6-580c9b07e0b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "train_news.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jsonid</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>speakerjobtitle</th>\n",
              "      <th>stateinfo</th>\n",
              "      <th>partyaffiliation</th>\n",
              "      <th>barelytruecounts</th>\n",
              "      <th>falsecounts</th>\n",
              "      <th>halftruecounts</th>\n",
              "      <th>mostlytrueocunts</th>\n",
              "      <th>pantsonfirecounts</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2635.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "      <td>dwayne-bohac</td>\n",
              "      <td>State representative</td>\n",
              "      <td>Texas</td>\n",
              "      <td>republican</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a mailer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10540.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "      <td>scott-surovell</td>\n",
              "      <td>State delegate</td>\n",
              "      <td>Virginia</td>\n",
              "      <td>democrat</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a floor speech.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>324.json</td>\n",
              "      <td>mostly-true</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>foreign-policy</td>\n",
              "      <td>barack-obama</td>\n",
              "      <td>President</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>democrat</td>\n",
              "      <td>70.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Denver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1123.json</td>\n",
              "      <td>false</td>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>health-care</td>\n",
              "      <td>blog-posting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>none</td>\n",
              "      <td>7.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>a news release</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9028.json</td>\n",
              "      <td>half-true</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>economy,jobs</td>\n",
              "      <td>charlie-crist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Florida</td>\n",
              "      <td>democrat</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>an interview on CNN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       jsonid        label  ... pantsonfirecounts              context\n",
              "0   2635.json        false  ...               0.0             a mailer\n",
              "1  10540.json    half-true  ...               0.0      a floor speech.\n",
              "2    324.json  mostly-true  ...               9.0               Denver\n",
              "3   1123.json        false  ...              44.0       a news release\n",
              "4   9028.json    half-true  ...               2.0  an interview on CNN\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwv_eE-CvtIA",
        "outputId": "3bdd9e67-1e24-471b-c1b2-6da7ede0370f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "train_news.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10240 entries, 0 to 10239\n",
            "Data columns (total 14 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   jsonid             10240 non-null  object \n",
            " 1   label              10240 non-null  object \n",
            " 2   text               10240 non-null  object \n",
            " 3   subject            10238 non-null  object \n",
            " 4   speaker            10238 non-null  object \n",
            " 5   speakerjobtitle    7343 non-null   object \n",
            " 6   stateinfo          8032 non-null   object \n",
            " 7   partyaffiliation   10238 non-null  object \n",
            " 8   barelytruecounts   10238 non-null  float64\n",
            " 9   falsecounts        10238 non-null  float64\n",
            " 10  halftruecounts     10238 non-null  float64\n",
            " 11  mostlytrueocunts   10238 non-null  float64\n",
            " 12  pantsonfirecounts  10238 non-null  float64\n",
            " 13  context            10138 non-null  object \n",
            "dtypes: float64(5), object(9)\n",
            "memory usage: 1.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YENSdMZjwFlE"
      },
      "source": [
        "## Politifact news dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oksu_sRQwI8w",
        "outputId": "6f3149e9-75d1-486e-f932-4db4ec00ebc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "politifact_fake = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/politifact_dataset/Fake.csv\",sep=',')\n",
        "politifact_fake['label'] = 1\n",
        "politifact_true = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/politifact_dataset/True.csv\",sep=',')\n",
        "politifact_true['label'] = 0\n",
        "df_politifact = pd.concat([politifact_fake, politifact_true])\n",
        "df_politifact.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... label\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...  ...     1\n",
              "1   Drunk Bragging Trump Staffer Started Russian ...  ...     1\n",
              "2   Sheriff David Clarke Becomes An Internet Joke...  ...     1\n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...  ...     1\n",
              "4   Pope Francis Just Called Out Donald Trump Dur...  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MinmO8AwLAv",
        "outputId": "76a6fcc4-45f3-4d98-96de-4474dcb92a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df_politifact.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 44898 entries, 0 to 21416\n",
            "Data columns (total 5 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   title    44898 non-null  object\n",
            " 1   text     44898 non-null  object\n",
            " 2   subject  44898 non-null  object\n",
            " 3   date     44898 non-null  object\n",
            " 4   label    44898 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 2.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brmQejKeXe2F"
      },
      "source": [
        "## Stance Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MN4hyKUXfPX"
      },
      "source": [
        "train_bodies = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/stance_dataset/train_bodies.csv\")\n",
        "train_stances = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/stance_dataset/train_stances.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImHQiTGmgnBT"
      },
      "source": [
        "test_bodies = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/stance_dataset/competition_test_bodies.csv\")\n",
        "test_stances = pd.read_csv(\"/content/drive/Shared drives/SheCodes/Datasets/stance_dataset/competition_test_stances.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxAGgnfrXmjN",
        "outputId": "8a8845a8-1f6f-480d-dcd9-8fd35db93bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "train_stances.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 49972 entries, 0 to 49971\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Headline  49972 non-null  object\n",
            " 1   Body ID   49972 non-null  int64 \n",
            " 2   Stance    49972 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSPzF9bRXqmI",
        "outputId": "c9a3705a-c75d-4a27-cfbd-4281980d14ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "train_bodies.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1683 entries, 0 to 1682\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Body ID      1683 non-null   int64 \n",
            " 1   articleBody  1683 non-null   object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 26.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb0w_jg4gwXq",
        "outputId": "51ad149f-bce6-4eaf-c541-5a6fe92e2f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "test_stances.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25413 entries, 0 to 25412\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Headline  25413 non-null  object\n",
            " 1   Body ID   25413 non-null  int64 \n",
            " 2   Stance    25413 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 595.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfWb53OJZTBj",
        "outputId": "2e887625-12e8-48fe-dab4-4471a2710080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_bodies.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>Posting photos of a gun-toting child online, I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID                                        articleBody\n",
              "0        0  A small meteorite crashed into a wooded area i...\n",
              "1        4  Last week we hinted at what was to come as Ebo...\n",
              "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
              "3        6  Posting photos of a gun-toting child online, I...\n",
              "4        7  At least 25 suspected Boko Haram insurgents we..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42scoYaPZoka",
        "outputId": "db8af1fb-1087-4e2a-d39a-ee11f0959a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_stances.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Police find mass graves with at least '15 bodi...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
              "      <td>158</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
              "      <td>137</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
              "      <td>1034</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
              "      <td>1923</td>\n",
              "      <td>disagree</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  Body ID     Stance\n",
              "0  Police find mass graves with at least '15 bodi...      712  unrelated\n",
              "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree\n",
              "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n",
              "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n",
              "4  Spider burrowed through tourist's stomach and ...     1923   disagree"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBADndU1cIH6"
      },
      "source": [
        "final_train_stances = pd.merge(left=train_stances, right=train_bodies, how='left', left_on='Body ID', right_on='Body ID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpm_JKOKey3X",
        "outputId": "80b3a49c-6826-4aeb-e162-2443d9e7971d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "final_train_stances.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Police find mass graves with at least '15 bodi...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
              "      <td>158</td>\n",
              "      <td>agree</td>\n",
              "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
              "      <td>137</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>30-year-old Moscow resident was hospitalized w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
              "      <td>1034</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>(Reuters) - A Canadian soldier was shot at the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
              "      <td>1923</td>\n",
              "      <td>disagree</td>\n",
              "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ...                                        articleBody\n",
              "0  Police find mass graves with at least '15 bodi...  ...  Danny Boyle is directing the untitled film\\n\\n...\n",
              "1  Hundreds of Palestinians flee floods in Gaza a...  ...  Hundreds of Palestinians were evacuated from t...\n",
              "2  Christian Bale passes on role of Steve Jobs, a...  ...  30-year-old Moscow resident was hospitalized w...\n",
              "3  HBO and Apple in Talks for $15/Month Apple TV ...  ...  (Reuters) - A Canadian soldier was shot at the...\n",
              "4  Spider burrowed through tourist's stomach and ...  ...  Fear not arachnophobes, the story of Bunbury's...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpP_i-wHe3qG",
        "outputId": "9f0a3873-0505-4413-a02d-1a0ce5ef60ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "final_train_stances.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Headline       0\n",
              "Body ID        0\n",
              "Stance         0\n",
              "articleBody    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKs5bV9eg8he"
      },
      "source": [
        "final_test_stances = pd.merge(left=test_stances, right=test_bodies, how='left', left_on='Body ID', right_on='Body ID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tscskpNhEIT",
        "outputId": "fd9949e0-4a54-4f73-db2e-e4f034bd9c5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "final_test_stances.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
              "      <td>2008</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A RESPECTED senior French police officer inves...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
              "      <td>1550</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>Dave Morin's social networking company Path is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
              "      <td>2</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
              "      <td>1793</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>Hewlett-Packard is officially splitting in two...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
              "      <td>37</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>An airline passenger headed to Dallas was remo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ...                                        articleBody\n",
              "0  Ferguson riots: Pregnant woman loses eye after...  ...  A RESPECTED senior French police officer inves...\n",
              "1  Crazy Conservatives Are Sure a Gitmo Detainee ...  ...  Dave Morin's social networking company Path is...\n",
              "2  A Russian Guy Says His Justin Bieber Ringtone ...  ...  A bereaved Afghan mother took revenge on the T...\n",
              "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...  ...  Hewlett-Packard is officially splitting in two...\n",
              "4  Argentina's President Adopts Boy to End Werewo...  ...  An airline passenger headed to Dallas was remo...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBOMN6GwhHwi",
        "outputId": "1f117bdc-045b-4e7e-a160-025efb4ff8e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "final_test_stances.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Headline       0\n",
              "Body ID        0\n",
              "Stance         0\n",
              "articleBody    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1fUqGc1jVGA"
      },
      "source": [
        "final_stance = pd.concat([final_train_stances, final_test_stances], axis=0, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3didRiuDmn3i",
        "outputId": "34a24231-ec65-45e7-bdf6-e58d24b09925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "final_stance.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Police find mass graves with at least '15 bodi...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
              "      <td>158</td>\n",
              "      <td>agree</td>\n",
              "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
              "      <td>137</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>30-year-old Moscow resident was hospitalized w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
              "      <td>1034</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>(Reuters) - A Canadian soldier was shot at the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
              "      <td>1923</td>\n",
              "      <td>disagree</td>\n",
              "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ...                                        articleBody\n",
              "0  Police find mass graves with at least '15 bodi...  ...  Danny Boyle is directing the untitled film\\n\\n...\n",
              "1  Hundreds of Palestinians flee floods in Gaza a...  ...  Hundreds of Palestinians were evacuated from t...\n",
              "2  Christian Bale passes on role of Steve Jobs, a...  ...  30-year-old Moscow resident was hospitalized w...\n",
              "3  HBO and Apple in Talks for $15/Month Apple TV ...  ...  (Reuters) - A Canadian soldier was shot at the...\n",
              "4  Spider burrowed through tourist's stomach and ...  ...  Fear not arachnophobes, the story of Bunbury's...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgtlkv4Bjmlb",
        "outputId": "5fa1ace1-e478-4293-db33-d85cfa307852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "final_stance.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Headline       0\n",
              "Body ID        0\n",
              "Stance         0\n",
              "articleBody    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VoMjT5r7dS",
        "outputId": "e9640b83-9767-4601-aed2-5f8c3b39a5f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "final_stance.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75385, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOnxj2x7lBCb",
        "outputId": "b143dbf3-4828-4cca-ee2f-80f2b4e42f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "final_stance.drop('Body ID', axis=1, inplace=True)\n",
        "final_stance.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 75385 entries, 0 to 75384\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Headline     75385 non-null  object\n",
            " 1   Stance       75385 non-null  object\n",
            " 2   articleBody  75385 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN9Z-T1Pjz05"
      },
      "source": [
        "final_stance.rename(columns={'Headline': 'title', 'articleBody': 'text', 'Stance': 'label'}, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjwen7ohlT3-",
        "outputId": "f9d7fd6c-f616-4755-ed28-a48b51d359ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "final_stance.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 75385 entries, 0 to 75384\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   title   75385 non-null  object\n",
            " 1   label   75385 non-null  object\n",
            " 2   text    75385 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvP_7GUqv0k5"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPYFQRTYv3-y"
      },
      "source": [
        "def cleaning(raw_news):\n",
        "    import nltk\n",
        "    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    # 4. Convert the stopwords list to \"set\" data type.\n",
        "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    \n",
        "    # 5. Remove stop words. \n",
        "    words = [w for w in  news_words  if not w in stops]\n",
        "    \n",
        "    # 6. Lemmentize \n",
        "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "    \n",
        "    # 7. Stemming\n",
        "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "    \n",
        "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "    return \" \".join(stems)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTPXgOpIb-k5",
        "outputId": "1c00f893-074f-42d1-e0cd-936e05bea26f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import time\n",
        "# clean training and test data \n",
        "# create new column \"tokenized\"\n",
        "t1 = time.time()\n",
        "\n",
        "# Add the processed data to the original data. \n",
        "# Perhaps using apply function would be more elegant and concise than using for loop\n",
        "train_news['clean'] = train_news[\"text\"].apply(cleaning) \n",
        "\n",
        "t2 = time.time()\n",
        "print(\"\\nTime to clean, tokenize and stem train data: \\n\", len(train_news), \"news:\", (t2-t1)/60, \"min\")\n",
        "\n",
        "t1 = time.time()\n",
        "test_news['clean'] = test_news[\"text\"].apply(cleaning)\n",
        "\n",
        "t2 = time.time()\n",
        "print(\"\\n\\nTime to clean, tokenize and stem test data: \\n\", len(test_news), \"news:\", (t2-t1)/60, \"min\")\n",
        "\n",
        "t1 = time.time()\n",
        "valid_news['clean'] = valid_news[\"text\"].apply(cleaning)\n",
        "\n",
        "t2 = time.time()\n",
        "print(\"\\n\\nTime to clean, tokenize and stem valid data: \\n\", len(valid_news), \"news:\", (t2-t1)/60, \"min\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Time to clean, tokenize and stem train data: \n",
            " 10240 news: 0.13514912923177083 min\n",
            "\n",
            "\n",
            "Time to clean, tokenize and stem test data: \n",
            " 1267 news: 0.013047262032826742 min\n",
            "\n",
            "\n",
            "Time to clean, tokenize and stem valid data: \n",
            " 1284 news: 0.013135802745819092 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbvDMp_cdACQ"
      },
      "source": [
        "# Feature 11: Title Vs Body \n",
        "**Name: Monica Dommaraju**\n",
        "\n",
        "\n",
        "My main idea of Title vs Body factor analysis is to pick Title and text features from each of these datasets and try to extract as many distilled features and then run models for classsification.\n",
        "\n",
        "1. Kaggle Fake News Dataset\n",
        "2. Liar-Liar Dataset\n",
        "3. politifact news Dataset\n",
        "4. Stance Dataset (from FakeNews Detection Challenge)\n",
        "\n",
        "\n",
        "`kaggle Fake News Dataset` Dataset and `Politifact news dataset` contains the title and text columns, but Liar-Liar Dataset contains only text column with out title text (I beleive `subject` feature in the Liar-Liar Dataset is not same the title). This made look for alternative datasets, so I can work on multi label classification. \n",
        "\n",
        "  I found Stance Dataset posted by FakeNews Detection\n",
        "\n",
        "\n",
        "and perform the following steps\n",
        "* Amalgamate all three datasets\n",
        "* cleaning the dataset \n",
        "* visualizing it using wordcloud\n",
        "* Calculate cosine similarity between Tile and Body for each sample\n",
        "* Calculate Number of matching strong words between these two columns\n",
        "* Run multiple classification models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD3gFuyAfhfL"
      },
      "source": [
        "## Amalgamation\n",
        "**Amalgamate Kaggle Fake News Dataset and Politifact News Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnLv9GDq6lII"
      },
      "source": [
        "df_kaggle = pd.concat([fake_train, fake_test])\n",
        "df_kaggle.shape\n",
        "# Pick only title and text columns from Kaggle dataset\n",
        "df_final = df_kaggle[['title', 'text', 'label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63wgvvKW8DUQ",
        "outputId": "6a28dc75-1401-4306-bdff-387f61bc5c26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_final = df_final.append(df_politifact[['title', 'text', 'label']])\n",
        "\n",
        "# Todo: Remove this sample technique when running the model finally\n",
        "df_final = df_final.sample(frac=0.03).reset_index(drop=True)\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...</td>\n",
              "      <td>Jay Dyer 21st Century WireJamie Hanshaw, autho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cyberattack on Belgian media outlets claimed b...</td>\n",
              "      <td>Mon, 24 Oct 2016 21:12 UTC © www,syrian-cyber-...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>France Presidential Election 2017 Livewire</td>\n",
              "      <td>Polls have closed for the second and final rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despite Republican’s Best Efforts, SCOTUS Jus...</td>\n",
              "      <td>Much to the dismay of so-called  pro-life  Rep...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNC Chair’s Comment Reveals Why Republicans M...</td>\n",
              "      <td>You almost have to feel sorry for Republicans ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... label\n",
              "0  HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...  ...     1\n",
              "1  Cyberattack on Belgian media outlets claimed b...  ...     1\n",
              "2         France Presidential Election 2017 Livewire  ...     0\n",
              "3   Despite Republican’s Best Efforts, SCOTUS Jus...  ...     1\n",
              "4   RNC Chair’s Comment Reveals Why Republicans M...  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4hh7XKO0VDg",
        "outputId": "20c5c93a-adec-4378-945f-34c27e7c2ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Todo: Remove this sample technique when running the model finally\n",
        "final_stance = final_stance.sample(frac=0.03).reset_index(drop=True)\n",
        "print(final_stance.shape)\n",
        "final_stance.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(754, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sadly, Pumpkin Spice Condoms Aren't A Thing Af...</td>\n",
              "      <td>agree</td>\n",
              "      <td>Durex denies rumors of a ‘pumpkin spice’ flavo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This Woman Claims To Have Had Plastic Surgery ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Justin Bieber Saved This Guy From a Bear Attack</td>\n",
              "      <td>discuss</td>\n",
              "      <td>When faced with the choice of feasting on a fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina president adopts young Jewish boy as...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ESPN host Chris Fowler was duped by a fake web...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Iraq claims ISIS leader al-Baghdadi’s wife has...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                               text\n",
              "0  Sadly, Pumpkin Spice Condoms Aren't A Thing Af...  ...  Durex denies rumors of a ‘pumpkin spice’ flavo...\n",
              "1  This Woman Claims To Have Had Plastic Surgery ...  ...  A bereaved Afghan mother took revenge on the T...\n",
              "2    Justin Bieber Saved This Guy From a Bear Attack  ...  When faced with the choice of feasting on a fi...\n",
              "3  Argentina president adopts young Jewish boy as...  ...  ESPN host Chris Fowler was duped by a fake web...\n",
              "4  Iraq claims ISIS leader al-Baghdadi’s wife has...  ...  There has been a shooting at the War Memorial ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl7YNe_j2duz"
      },
      "source": [
        "## Clean the title and text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJPMlwWI2cdd",
        "outputId": "04876208-abd6-4663-e872-89944bc0659b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Cleaning on Fake News Dataset\n",
        "final_stance['cleaned_title'] = final_stance[\"title\"].map(lambda x: cleaning(x))\n",
        "final_stance['cleaned_text'] = final_stance[\"text\"].map(lambda x: cleaning(x))\n",
        "final_stance.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sadly, Pumpkin Spice Condoms Aren't A Thing Af...</td>\n",
              "      <td>agree</td>\n",
              "      <td>Durex denies rumors of a ‘pumpkin spice’ flavo...</td>\n",
              "      <td>sad pumpkin spice condom thing</td>\n",
              "      <td>durex deni rumor pumpkin spice flavor condom o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This Woman Claims To Have Had Plastic Surgery ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "      <td>woman claim plastic surgeri third breast</td>\n",
              "      <td>bereav afghan mother took reveng taliban watch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Justin Bieber Saved This Guy From a Bear Attack</td>\n",
              "      <td>discuss</td>\n",
              "      <td>When faced with the choice of feasting on a fi...</td>\n",
              "      <td>justin bieber save guy bear attack</td>\n",
              "      <td>face choic feast fine meal human listen justin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina president adopts young Jewish boy as...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ESPN host Chris Fowler was duped by a fake web...</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>espn host chris fowler dupe fake websit mediam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Iraq claims ISIS leader al-Baghdadi’s wife has...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>shoot war memori parliament hill unconfirm rep...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                       cleaned_text\n",
              "0  Sadly, Pumpkin Spice Condoms Aren't A Thing Af...  ...  durex deni rumor pumpkin spice flavor condom o...\n",
              "1  This Woman Claims To Have Had Plastic Surgery ...  ...  bereav afghan mother took reveng taliban watch...\n",
              "2    Justin Bieber Saved This Guy From a Bear Attack  ...  face choic feast fine meal human listen justin...\n",
              "3  Argentina president adopts young Jewish boy as...  ...  espn host chris fowler dupe fake websit mediam...\n",
              "4  Iraq claims ISIS leader al-Baghdadi’s wife has...  ...  shoot war memori parliament hill unconfirm rep...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09dhiP1kLPuP",
        "outputId": "766d685e-fbfc-4e81-cb75-57b995426f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# Cleaning on Amalamated Kaggle and Fake News dataset.\n",
        "print (final_stance.info())\n",
        "print (df_final.info())\n",
        "df_final['title'] = df_final['title'].astype('str')\n",
        "df_final['text'] = df_final['text'].astype('str')\n",
        "df_final['cleaned_title'] = df_final['title'].astype('str').map(lambda x: cleaning(x))\n",
        "df_final['cleaned_text'] = df_final['text'].astype('str').map(lambda x: cleaning(x))\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 754 entries, 0 to 753\n",
            "Data columns (total 5 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   title          754 non-null    object\n",
            " 1   label          754 non-null    object\n",
            " 2   text           754 non-null    object\n",
            " 3   cleaned_title  754 non-null    object\n",
            " 4   cleaned_text   754 non-null    object\n",
            "dtypes: object(5)\n",
            "memory usage: 29.6+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 709 entries, 0 to 708\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   title   702 non-null    object\n",
            " 1   text    709 non-null    object\n",
            " 2   label   709 non-null    int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 16.7+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...</td>\n",
              "      <td>Jay Dyer 21st Century WireJamie Hanshaw, autho...</td>\n",
              "      <td>1</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch</td>\n",
              "      <td>jay dyer st centuri wirejami hanshaw author op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cyberattack on Belgian media outlets claimed b...</td>\n",
              "      <td>Mon, 24 Oct 2016 21:12 UTC © www,syrian-cyber-...</td>\n",
              "      <td>1</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>mon oct utc www syrian cyber armi net sever be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>France Presidential Election 2017 Livewire</td>\n",
              "      <td>Polls have closed for the second and final rou...</td>\n",
              "      <td>0</td>\n",
              "      <td>franc presidenti elect livewir</td>\n",
              "      <td>poll close second final round vote french pres...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despite Republican’s Best Efforts, SCOTUS Jus...</td>\n",
              "      <td>Much to the dismay of so-called  pro-life  Rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>much dismay call pro life republican suprem co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNC Chair’s Comment Reveals Why Republicans M...</td>\n",
              "      <td>You almost have to feel sorry for Republicans ...</td>\n",
              "      <td>1</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>almost feel sorri republican almost find reall...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                       cleaned_text\n",
              "0  HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...  ...  jay dyer st centuri wirejami hanshaw author op...\n",
              "1  Cyberattack on Belgian media outlets claimed b...  ...  mon oct utc www syrian cyber armi net sever be...\n",
              "2         France Presidential Election 2017 Livewire  ...  poll close second final round vote french pres...\n",
              "3   Despite Republican’s Best Efforts, SCOTUS Jus...  ...  much dismay call pro life republican suprem co...\n",
              "4   RNC Chair’s Comment Reveals Why Republicans M...  ...  almost feel sorri republican almost find reall...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YT0K144u4Sn"
      },
      "source": [
        "##Count of sentences in Title and Text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i23-04X1vCEs"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def count_sentences(data):\n",
        "  data['count_title_sentences'] = data['title'].apply(lambda x: len(sent_tokenize(x)))\n",
        "  data['count_text_sentences'] = data['text'].apply(lambda x: len(sent_tokenize(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih6qUaxKwKEs",
        "outputId": "0785ac10-a9d6-46ee-ca35-d5652dac6ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Calculate the total number of sentences in Title and Text on Multiclass labeled Dataset\n",
        "count_sentences(final_stance)\n",
        "final_stance.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sadly, Pumpkin Spice Condoms Aren't A Thing Af...</td>\n",
              "      <td>agree</td>\n",
              "      <td>Durex denies rumors of a ‘pumpkin spice’ flavo...</td>\n",
              "      <td>sad pumpkin spice condom thing</td>\n",
              "      <td>durex deni rumor pumpkin spice flavor condom o...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This Woman Claims To Have Had Plastic Surgery ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "      <td>woman claim plastic surgeri third breast</td>\n",
              "      <td>bereav afghan mother took reveng taliban watch...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Justin Bieber Saved This Guy From a Bear Attack</td>\n",
              "      <td>discuss</td>\n",
              "      <td>When faced with the choice of feasting on a fi...</td>\n",
              "      <td>justin bieber save guy bear attack</td>\n",
              "      <td>face choic feast fine meal human listen justin...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina president adopts young Jewish boy as...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ESPN host Chris Fowler was duped by a fake web...</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>espn host chris fowler dupe fake websit mediam...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Iraq claims ISIS leader al-Baghdadi’s wife has...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>shoot war memori parliament hill unconfirm rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... count_text_sentences\n",
              "0  Sadly, Pumpkin Spice Condoms Aren't A Thing Af...  ...                   10\n",
              "1  This Woman Claims To Have Had Plastic Surgery ...  ...                   14\n",
              "2    Justin Bieber Saved This Guy From a Bear Attack  ...                    6\n",
              "3  Argentina president adopts young Jewish boy as...  ...                   18\n",
              "4  Iraq claims ISIS leader al-Baghdadi’s wife has...  ...                    2\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jriusj0LwS3",
        "outputId": "5183dd43-9dd8-4920-cd2f-6d5bc5b446a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# Calculate the total number of sentences in Title and Text on Binary class labeled Dataset\n",
        "count_sentences(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...</td>\n",
              "      <td>Jay Dyer 21st Century WireJamie Hanshaw, autho...</td>\n",
              "      <td>1</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch</td>\n",
              "      <td>jay dyer st centuri wirejami hanshaw author op...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cyberattack on Belgian media outlets claimed b...</td>\n",
              "      <td>Mon, 24 Oct 2016 21:12 UTC © www,syrian-cyber-...</td>\n",
              "      <td>1</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>mon oct utc www syrian cyber armi net sever be...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>France Presidential Election 2017 Livewire</td>\n",
              "      <td>Polls have closed for the second and final rou...</td>\n",
              "      <td>0</td>\n",
              "      <td>franc presidenti elect livewir</td>\n",
              "      <td>poll close second final round vote french pres...</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despite Republican’s Best Efforts, SCOTUS Jus...</td>\n",
              "      <td>Much to the dismay of so-called  pro-life  Rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>much dismay call pro life republican suprem co...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNC Chair’s Comment Reveals Why Republicans M...</td>\n",
              "      <td>You almost have to feel sorry for Republicans ...</td>\n",
              "      <td>1</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>almost feel sorri republican almost find reall...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... count_text_sentences\n",
              "0  HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...  ...                    2\n",
              "1  Cyberattack on Belgian media outlets claimed b...  ...                   18\n",
              "2         France Presidential Election 2017 Livewire  ...                    7\n",
              "3   Despite Republican’s Best Efforts, SCOTUS Jus...  ...                    8\n",
              "4   RNC Chair’s Comment Reveals Why Republicans M...  ...                   16\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny7a9yBAjUH-"
      },
      "source": [
        "## Generating N-Grams and Their Lengths\n",
        "\n",
        "Uni, Bi and Tri grams are generated so we can extract some count fearures for each of these grams along with the count features of sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ02lCv9zGLu"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def ngram(text, n):\n",
        "    n_grams = ngrams(word_tokenize(text), n)\n",
        "    return [ '_'.join(grams) for grams in n_grams]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiLDipsLf_VA"
      },
      "source": [
        "def generate_ngram_features(data):\n",
        "  data[\"title_unigram\"] = data[\"cleaned_title\"].map(lambda x: ngram(x, 1))\n",
        "  data[\"text_unigram\"] = data[\"cleaned_text\"].map(lambda x: ngram(x, 1))\n",
        "  data[\"count_title_unigram\"] = list(data.apply(lambda x: len(x['title_unigram']), axis=1))\n",
        "  data[\"count_text_unigram\"] = list(data.apply(lambda x: len(x['text_unigram']), axis=1))\n",
        "  data[\"unique_count_title_unigram\"] = list(data.apply(lambda x: len(set(x['title_unigram'])), axis=1))\n",
        "  data[\"unique_count_text_unigram\"] = list(data.apply(lambda x: len(set(x['text_unigram'])), axis=1))\n",
        "\n",
        "  data[\"title_bigram\"] = data[\"cleaned_title\"].map(lambda x: ngram(x, 2))\n",
        "  data[\"text_bigram\"] = data[\"cleaned_text\"].map(lambda x: ngram(x, 2))\n",
        "  data[\"count_title_bigram\"] = list(data.apply(lambda x: len(x['title_bigram']), axis=1))\n",
        "  data[\"count_text_bigram\"] = list(data.apply(lambda x: len(x['text_bigram']), axis=1))\n",
        "  data[\"unique_count_title_bigram\"] = list(data.apply(lambda x: len(set(x['title_bigram'])), axis=1))\n",
        "  data[\"unique_count_text_bigram\"] = list(data.apply(lambda x: len(set(x['text_bigram'])), axis=1))\n",
        "\n",
        "  data[\"title_trigram\"] = data[\"cleaned_title\"].map(lambda x: ngram(x, 3))\n",
        "  data[\"text_trigram\"] = data[\"cleaned_text\"].map(lambda x: ngram(x, 3))\n",
        "  data[\"count_title_trigram\"] = list(data.apply(lambda x: len(x['title_trigram']), axis=1))\n",
        "  data[\"count_text_trigram\"] = list(data.apply(lambda x: len(x['text_trigram']), axis=1))\n",
        "  data[\"unique_count_title_trigram\"] = list(data.apply(lambda x: len(set(x['title_trigram'])), axis=1))\n",
        "  data[\"unique_count_text_trigram\"] = list(data.apply(lambda x: len(set(x['text_trigram'])), axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIMY2VMOzoGf",
        "outputId": "09130d80-6035-48da-b8a6-28b49db068b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "# Generate n-grams of title and text. Then calculate the length of unigram, bigram and trigrams on Multiclass labeled DS\n",
        "generate_ngram_features(final_stance)\n",
        "final_stance.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sadly, Pumpkin Spice Condoms Aren't A Thing Af...</td>\n",
              "      <td>agree</td>\n",
              "      <td>Durex denies rumors of a ‘pumpkin spice’ flavo...</td>\n",
              "      <td>sad pumpkin spice condom thing</td>\n",
              "      <td>durex deni rumor pumpkin spice flavor condom o...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>[sad, pumpkin, spice, condom, thing]</td>\n",
              "      <td>[durex, deni, rumor, pumpkin, spice, flavor, c...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>[sad_pumpkin, pumpkin_spice, spice_condom, con...</td>\n",
              "      <td>[durex_deni, deni_rumor, rumor_pumpkin, pumpki...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>84</td>\n",
              "      <td>[sad_pumpkin_spice, pumpkin_spice_condom, spic...</td>\n",
              "      <td>[durex_deni_rumor, deni_rumor_pumpkin, rumor_p...</td>\n",
              "      <td>3</td>\n",
              "      <td>92</td>\n",
              "      <td>3</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This Woman Claims To Have Had Plastic Surgery ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "      <td>woman claim plastic surgeri third breast</td>\n",
              "      <td>bereav afghan mother took reveng taliban watch...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>[woman, claim, plastic, surgeri, third, breast]</td>\n",
              "      <td>[bereav, afghan, mother, took, reveng, taliban...</td>\n",
              "      <td>6</td>\n",
              "      <td>180</td>\n",
              "      <td>6</td>\n",
              "      <td>138</td>\n",
              "      <td>[woman_claim, claim_plastic, plastic_surgeri, ...</td>\n",
              "      <td>[bereav_afghan, afghan_mother, mother_took, to...</td>\n",
              "      <td>5</td>\n",
              "      <td>179</td>\n",
              "      <td>5</td>\n",
              "      <td>175</td>\n",
              "      <td>[woman_claim_plastic, claim_plastic_surgeri, p...</td>\n",
              "      <td>[bereav_afghan_mother, afghan_mother_took, mot...</td>\n",
              "      <td>4</td>\n",
              "      <td>178</td>\n",
              "      <td>4</td>\n",
              "      <td>177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Justin Bieber Saved This Guy From a Bear Attack</td>\n",
              "      <td>discuss</td>\n",
              "      <td>When faced with the choice of feasting on a fi...</td>\n",
              "      <td>justin bieber save guy bear attack</td>\n",
              "      <td>face choic feast fine meal human listen justin...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[justin, bieber, save, guy, bear, attack]</td>\n",
              "      <td>[face, choic, feast, fine, meal, human, listen...</td>\n",
              "      <td>6</td>\n",
              "      <td>83</td>\n",
              "      <td>6</td>\n",
              "      <td>76</td>\n",
              "      <td>[justin_bieber, bieber_save, save_guy, guy_bea...</td>\n",
              "      <td>[face_choic, choic_feast, feast_fine, fine_mea...</td>\n",
              "      <td>5</td>\n",
              "      <td>82</td>\n",
              "      <td>5</td>\n",
              "      <td>81</td>\n",
              "      <td>[justin_bieber_save, bieber_save_guy, save_guy...</td>\n",
              "      <td>[face_choic_feast, choic_feast_fine, feast_fin...</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina president adopts young Jewish boy as...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ESPN host Chris Fowler was duped by a fake web...</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>espn host chris fowler dupe fake websit mediam...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[argentina, presid, adopt, young, jewish, boy,...</td>\n",
              "      <td>[espn, host, chris, fowler, dupe, fake, websit...</td>\n",
              "      <td>10</td>\n",
              "      <td>265</td>\n",
              "      <td>10</td>\n",
              "      <td>165</td>\n",
              "      <td>[argentina_presid, presid_adopt, adopt_young, ...</td>\n",
              "      <td>[espn_host, host_chris, chris_fowler, fowler_d...</td>\n",
              "      <td>9</td>\n",
              "      <td>264</td>\n",
              "      <td>9</td>\n",
              "      <td>242</td>\n",
              "      <td>[argentina_presid_adopt, presid_adopt_young, a...</td>\n",
              "      <td>[espn_host_chris, host_chris_fowler, chris_fow...</td>\n",
              "      <td>8</td>\n",
              "      <td>263</td>\n",
              "      <td>8</td>\n",
              "      <td>258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Iraq claims ISIS leader al-Baghdadi’s wife has...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>shoot war memori parliament hill unconfirm rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[iraq, claim, isi, leader, al, baghdadi, wife,...</td>\n",
              "      <td>[shoot, war, memori, parliament, hill, unconfi...</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>[iraq_claim, claim_isi, isi_leader, leader_al,...</td>\n",
              "      <td>[shoot_war, war_memori, memori_parliament, par...</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>[iraq_claim_isi, claim_isi_leader, isi_leader_...</td>\n",
              "      <td>[shoot_war_memori, war_memori_parliament, memo...</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... unique_count_text_trigram\n",
              "0  Sadly, Pumpkin Spice Condoms Aren't A Thing Af...  ...                        87\n",
              "1  This Woman Claims To Have Had Plastic Surgery ...  ...                       177\n",
              "2    Justin Bieber Saved This Guy From a Bear Attack  ...                        81\n",
              "3  Argentina president adopts young Jewish boy as...  ...                       258\n",
              "4  Iraq claims ISIS leader al-Baghdadi’s wife has...  ...                         8\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGaaIKESMAjJ",
        "outputId": "78f0217e-4adf-4b32-ef97-ddf01fa916df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Generate n-grams of title and text. Then calculate the length of unigram, bigram and trigrams on Binary class labeled DS\n",
        "generate_ngram_features(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...</td>\n",
              "      <td>Jay Dyer 21st Century WireJamie Hanshaw, autho...</td>\n",
              "      <td>1</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch</td>\n",
              "      <td>jay dyer st centuri wirejami hanshaw author op...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[hollywood, witchcraft, dark, side, reveal, wi...</td>\n",
              "      <td>[jay, dyer, st, centuri, wirejami, hanshaw, au...</td>\n",
              "      <td>6</td>\n",
              "      <td>95</td>\n",
              "      <td>6</td>\n",
              "      <td>74</td>\n",
              "      <td>[hollywood_witchcraft, witchcraft_dark, dark_s...</td>\n",
              "      <td>[jay_dyer, dyer_st, st_centuri, centuri_wireja...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>91</td>\n",
              "      <td>[hollywood_witchcraft_dark, witchcraft_dark_si...</td>\n",
              "      <td>[jay_dyer_st, dyer_st_centuri, st_centuri_wire...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cyberattack on Belgian media outlets claimed b...</td>\n",
              "      <td>Mon, 24 Oct 2016 21:12 UTC © www,syrian-cyber-...</td>\n",
              "      <td>1</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>mon oct utc www syrian cyber armi net sever be...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[cyberattack, belgian, medium, outlet, claim, ...</td>\n",
              "      <td>[mon, oct, utc, www, syrian, cyber, armi, net,...</td>\n",
              "      <td>8</td>\n",
              "      <td>252</td>\n",
              "      <td>8</td>\n",
              "      <td>180</td>\n",
              "      <td>[cyberattack_belgian, belgian_medium, medium_o...</td>\n",
              "      <td>[mon_oct, oct_utc, utc_www, www_syrian, syrian...</td>\n",
              "      <td>7</td>\n",
              "      <td>251</td>\n",
              "      <td>7</td>\n",
              "      <td>240</td>\n",
              "      <td>[cyberattack_belgian_medium, belgian_medium_ou...</td>\n",
              "      <td>[mon_oct_utc, oct_utc_www, utc_www_syrian, www...</td>\n",
              "      <td>6</td>\n",
              "      <td>250</td>\n",
              "      <td>6</td>\n",
              "      <td>249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>France Presidential Election 2017 Livewire</td>\n",
              "      <td>Polls have closed for the second and final rou...</td>\n",
              "      <td>0</td>\n",
              "      <td>franc presidenti elect livewir</td>\n",
              "      <td>poll close second final round vote french pres...</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>[franc, presidenti, elect, livewir]</td>\n",
              "      <td>[poll, close, second, final, round, vote, fren...</td>\n",
              "      <td>4</td>\n",
              "      <td>74</td>\n",
              "      <td>4</td>\n",
              "      <td>65</td>\n",
              "      <td>[franc_presidenti, presidenti_elect, elect_liv...</td>\n",
              "      <td>[poll_close, close_second, second_final, final...</td>\n",
              "      <td>3</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>71</td>\n",
              "      <td>[franc_presidenti_elect, presidenti_elect_live...</td>\n",
              "      <td>[poll_close_second, close_second_final, second...</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despite Republican’s Best Efforts, SCOTUS Jus...</td>\n",
              "      <td>Much to the dismay of so-called  pro-life  Rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>much dismay call pro life republican suprem co...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[despit, republican, best, effort, scotus, all...</td>\n",
              "      <td>[much, dismay, call, pro, life, republican, su...</td>\n",
              "      <td>10</td>\n",
              "      <td>210</td>\n",
              "      <td>10</td>\n",
              "      <td>133</td>\n",
              "      <td>[despit_republican, republican_best, best_effo...</td>\n",
              "      <td>[much_dismay, dismay_call, call_pro, pro_life,...</td>\n",
              "      <td>9</td>\n",
              "      <td>209</td>\n",
              "      <td>9</td>\n",
              "      <td>194</td>\n",
              "      <td>[despit_republican_best, republican_best_effor...</td>\n",
              "      <td>[much_dismay_call, dismay_call_pro, call_pro_l...</td>\n",
              "      <td>8</td>\n",
              "      <td>208</td>\n",
              "      <td>8</td>\n",
              "      <td>206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNC Chair’s Comment Reveals Why Republicans M...</td>\n",
              "      <td>You almost have to feel sorry for Republicans ...</td>\n",
              "      <td>1</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>almost feel sorri republican almost find reall...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[rnc, chair, comment, reveal, republican, may,...</td>\n",
              "      <td>[almost, feel, sorri, republican, almost, find...</td>\n",
              "      <td>9</td>\n",
              "      <td>289</td>\n",
              "      <td>9</td>\n",
              "      <td>210</td>\n",
              "      <td>[rnc_chair, chair_comment, comment_reveal, rev...</td>\n",
              "      <td>[almost_feel, feel_sorri, sorri_republican, re...</td>\n",
              "      <td>8</td>\n",
              "      <td>288</td>\n",
              "      <td>8</td>\n",
              "      <td>282</td>\n",
              "      <td>[rnc_chair_comment, chair_comment_reveal, comm...</td>\n",
              "      <td>[almost_feel_sorri, feel_sorri_republican, sor...</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... unique_count_text_trigram\n",
              "0  HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...  ...                        93\n",
              "1  Cyberattack on Belgian media outlets claimed b...  ...                       249\n",
              "2         France Presidential Election 2017 Livewire  ...                        72\n",
              "3   Despite Republican’s Best Efforts, SCOTUS Jus...  ...                       206\n",
              "4   RNC Chair’s Comment Reveals Why Republicans M...  ...                       287\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMbs960ppppn"
      },
      "source": [
        "## Common N-grams between Title and Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTd7_gfWqhiC"
      },
      "source": [
        "def common_ngrams_in_text(data):\n",
        "  data[\"count_title_unigrams_in_text\"] =  list(data.apply(lambda x: sum([1. for w in x['title_unigram'] if w in set(x['text_unigram'])]), axis=1))\n",
        "  data[\"count_title_bigrams_in_text\"] =  list(data.apply(lambda x: sum([1. for w in x['title_bigram'] if w in set(x['text_bigram'])]), axis=1))\n",
        "  data[\"count_title_trigrams_in_text\"] =  list(data.apply(lambda x: sum([1. for w in x['title_trigram'] if w in set(x['text_trigram'])]), axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAYK6twTzoU7",
        "outputId": "5ad3667f-73c4-46e5-8dfc-a3f1afd34947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "# Calculate the total number of common title n-grams in the text on Multi class labeled DS\n",
        "common_ngrams_in_text(final_stance)\n",
        "final_stance.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sadly, Pumpkin Spice Condoms Aren't A Thing Af...</td>\n",
              "      <td>agree</td>\n",
              "      <td>Durex denies rumors of a ‘pumpkin spice’ flavo...</td>\n",
              "      <td>sad pumpkin spice condom thing</td>\n",
              "      <td>durex deni rumor pumpkin spice flavor condom o...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>[sad, pumpkin, spice, condom, thing]</td>\n",
              "      <td>[durex, deni, rumor, pumpkin, spice, flavor, c...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>[sad_pumpkin, pumpkin_spice, spice_condom, con...</td>\n",
              "      <td>[durex_deni, deni_rumor, rumor_pumpkin, pumpki...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>84</td>\n",
              "      <td>[sad_pumpkin_spice, pumpkin_spice_condom, spic...</td>\n",
              "      <td>[durex_deni_rumor, deni_rumor_pumpkin, rumor_p...</td>\n",
              "      <td>3</td>\n",
              "      <td>92</td>\n",
              "      <td>3</td>\n",
              "      <td>87</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This Woman Claims To Have Had Plastic Surgery ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "      <td>woman claim plastic surgeri third breast</td>\n",
              "      <td>bereav afghan mother took reveng taliban watch...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>[woman, claim, plastic, surgeri, third, breast]</td>\n",
              "      <td>[bereav, afghan, mother, took, reveng, taliban...</td>\n",
              "      <td>6</td>\n",
              "      <td>180</td>\n",
              "      <td>6</td>\n",
              "      <td>138</td>\n",
              "      <td>[woman_claim, claim_plastic, plastic_surgeri, ...</td>\n",
              "      <td>[bereav_afghan, afghan_mother, mother_took, to...</td>\n",
              "      <td>5</td>\n",
              "      <td>179</td>\n",
              "      <td>5</td>\n",
              "      <td>175</td>\n",
              "      <td>[woman_claim_plastic, claim_plastic_surgeri, p...</td>\n",
              "      <td>[bereav_afghan_mother, afghan_mother_took, mot...</td>\n",
              "      <td>4</td>\n",
              "      <td>178</td>\n",
              "      <td>4</td>\n",
              "      <td>177</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Justin Bieber Saved This Guy From a Bear Attack</td>\n",
              "      <td>discuss</td>\n",
              "      <td>When faced with the choice of feasting on a fi...</td>\n",
              "      <td>justin bieber save guy bear attack</td>\n",
              "      <td>face choic feast fine meal human listen justin...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[justin, bieber, save, guy, bear, attack]</td>\n",
              "      <td>[face, choic, feast, fine, meal, human, listen...</td>\n",
              "      <td>6</td>\n",
              "      <td>83</td>\n",
              "      <td>6</td>\n",
              "      <td>76</td>\n",
              "      <td>[justin_bieber, bieber_save, save_guy, guy_bea...</td>\n",
              "      <td>[face_choic, choic_feast, feast_fine, fine_mea...</td>\n",
              "      <td>5</td>\n",
              "      <td>82</td>\n",
              "      <td>5</td>\n",
              "      <td>81</td>\n",
              "      <td>[justin_bieber_save, bieber_save_guy, save_guy...</td>\n",
              "      <td>[face_choic_feast, choic_feast_fine, feast_fin...</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina president adopts young Jewish boy as...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ESPN host Chris Fowler was duped by a fake web...</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>espn host chris fowler dupe fake websit mediam...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[argentina, presid, adopt, young, jewish, boy,...</td>\n",
              "      <td>[espn, host, chris, fowler, dupe, fake, websit...</td>\n",
              "      <td>10</td>\n",
              "      <td>265</td>\n",
              "      <td>10</td>\n",
              "      <td>165</td>\n",
              "      <td>[argentina_presid, presid_adopt, adopt_young, ...</td>\n",
              "      <td>[espn_host, host_chris, chris_fowler, fowler_d...</td>\n",
              "      <td>9</td>\n",
              "      <td>264</td>\n",
              "      <td>9</td>\n",
              "      <td>242</td>\n",
              "      <td>[argentina_presid_adopt, presid_adopt_young, a...</td>\n",
              "      <td>[espn_host_chris, host_chris_fowler, chris_fow...</td>\n",
              "      <td>8</td>\n",
              "      <td>263</td>\n",
              "      <td>8</td>\n",
              "      <td>258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Iraq claims ISIS leader al-Baghdadi’s wife has...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>shoot war memori parliament hill unconfirm rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[iraq, claim, isi, leader, al, baghdadi, wife,...</td>\n",
              "      <td>[shoot, war, memori, parliament, hill, unconfi...</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>[iraq_claim, claim_isi, isi_leader, leader_al,...</td>\n",
              "      <td>[shoot_war, war_memori, memori_parliament, par...</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>[iraq_claim_isi, claim_isi_leader, isi_leader_...</td>\n",
              "      <td>[shoot_war_memori, war_memori_parliament, memo...</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... count_title_trigrams_in_text\n",
              "0  Sadly, Pumpkin Spice Condoms Aren't A Thing Af...  ...                          1.0\n",
              "1  This Woman Claims To Have Had Plastic Surgery ...  ...                          0.0\n",
              "2    Justin Bieber Saved This Guy From a Bear Attack  ...                          0.0\n",
              "3  Argentina president adopts young Jewish boy as...  ...                          0.0\n",
              "4  Iraq claims ISIS leader al-Baghdadi’s wife has...  ...                          0.0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bR6p2anMZHh",
        "outputId": "6518d543-432c-48e2-a711-aa547122bcf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# Calculate the total number of common title n-grams in the text on Binary class labeled DS\n",
        "common_ngrams_in_text(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...</td>\n",
              "      <td>Jay Dyer 21st Century WireJamie Hanshaw, autho...</td>\n",
              "      <td>1</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch</td>\n",
              "      <td>jay dyer st centuri wirejami hanshaw author op...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[hollywood, witchcraft, dark, side, reveal, wi...</td>\n",
              "      <td>[jay, dyer, st, centuri, wirejami, hanshaw, au...</td>\n",
              "      <td>6</td>\n",
              "      <td>95</td>\n",
              "      <td>6</td>\n",
              "      <td>74</td>\n",
              "      <td>[hollywood_witchcraft, witchcraft_dark, dark_s...</td>\n",
              "      <td>[jay_dyer, dyer_st, st_centuri, centuri_wireja...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>91</td>\n",
              "      <td>[hollywood_witchcraft_dark, witchcraft_dark_si...</td>\n",
              "      <td>[jay_dyer_st, dyer_st_centuri, st_centuri_wire...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cyberattack on Belgian media outlets claimed b...</td>\n",
              "      <td>Mon, 24 Oct 2016 21:12 UTC © www,syrian-cyber-...</td>\n",
              "      <td>1</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>mon oct utc www syrian cyber armi net sever be...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[cyberattack, belgian, medium, outlet, claim, ...</td>\n",
              "      <td>[mon, oct, utc, www, syrian, cyber, armi, net,...</td>\n",
              "      <td>8</td>\n",
              "      <td>252</td>\n",
              "      <td>8</td>\n",
              "      <td>180</td>\n",
              "      <td>[cyberattack_belgian, belgian_medium, medium_o...</td>\n",
              "      <td>[mon_oct, oct_utc, utc_www, www_syrian, syrian...</td>\n",
              "      <td>7</td>\n",
              "      <td>251</td>\n",
              "      <td>7</td>\n",
              "      <td>240</td>\n",
              "      <td>[cyberattack_belgian_medium, belgian_medium_ou...</td>\n",
              "      <td>[mon_oct_utc, oct_utc_www, utc_www_syrian, www...</td>\n",
              "      <td>6</td>\n",
              "      <td>250</td>\n",
              "      <td>6</td>\n",
              "      <td>249</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>France Presidential Election 2017 Livewire</td>\n",
              "      <td>Polls have closed for the second and final rou...</td>\n",
              "      <td>0</td>\n",
              "      <td>franc presidenti elect livewir</td>\n",
              "      <td>poll close second final round vote french pres...</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>[franc, presidenti, elect, livewir]</td>\n",
              "      <td>[poll, close, second, final, round, vote, fren...</td>\n",
              "      <td>4</td>\n",
              "      <td>74</td>\n",
              "      <td>4</td>\n",
              "      <td>65</td>\n",
              "      <td>[franc_presidenti, presidenti_elect, elect_liv...</td>\n",
              "      <td>[poll_close, close_second, second_final, final...</td>\n",
              "      <td>3</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>71</td>\n",
              "      <td>[franc_presidenti_elect, presidenti_elect_live...</td>\n",
              "      <td>[poll_close_second, close_second_final, second...</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despite Republican’s Best Efforts, SCOTUS Jus...</td>\n",
              "      <td>Much to the dismay of so-called  pro-life  Rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>much dismay call pro life republican suprem co...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[despit, republican, best, effort, scotus, all...</td>\n",
              "      <td>[much, dismay, call, pro, life, republican, su...</td>\n",
              "      <td>10</td>\n",
              "      <td>210</td>\n",
              "      <td>10</td>\n",
              "      <td>133</td>\n",
              "      <td>[despit_republican, republican_best, best_effo...</td>\n",
              "      <td>[much_dismay, dismay_call, call_pro, pro_life,...</td>\n",
              "      <td>9</td>\n",
              "      <td>209</td>\n",
              "      <td>9</td>\n",
              "      <td>194</td>\n",
              "      <td>[despit_republican_best, republican_best_effor...</td>\n",
              "      <td>[much_dismay_call, dismay_call_pro, call_pro_l...</td>\n",
              "      <td>8</td>\n",
              "      <td>208</td>\n",
              "      <td>8</td>\n",
              "      <td>206</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNC Chair’s Comment Reveals Why Republicans M...</td>\n",
              "      <td>You almost have to feel sorry for Republicans ...</td>\n",
              "      <td>1</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>almost feel sorri republican almost find reall...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[rnc, chair, comment, reveal, republican, may,...</td>\n",
              "      <td>[almost, feel, sorri, republican, almost, find...</td>\n",
              "      <td>9</td>\n",
              "      <td>289</td>\n",
              "      <td>9</td>\n",
              "      <td>210</td>\n",
              "      <td>[rnc_chair, chair_comment, comment_reveal, rev...</td>\n",
              "      <td>[almost_feel, feel_sorri, sorri_republican, re...</td>\n",
              "      <td>8</td>\n",
              "      <td>288</td>\n",
              "      <td>8</td>\n",
              "      <td>282</td>\n",
              "      <td>[rnc_chair_comment, chair_comment_reveal, comm...</td>\n",
              "      <td>[almost_feel_sorri, feel_sorri_republican, sor...</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... count_title_trigrams_in_text\n",
              "0  HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...  ...                          0.0\n",
              "1  Cyberattack on Belgian media outlets claimed b...  ...                          2.0\n",
              "2         France Presidential Election 2017 Livewire  ...                          0.0\n",
              "3   Despite Republican’s Best Efforts, SCOTUS Jus...  ...                          3.0\n",
              "4   RNC Chair’s Comment Reveals Why Republicans M...  ...                          0.0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fdnUxbcc8Ky"
      },
      "source": [
        "## TF_IDF and Cosine Similarities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ_095uI6jSC"
      },
      "source": [
        "def concat_title_text(data):\n",
        "  data['cleaned_title_text'] = data['cleaned_title'] + ' ' + data['cleaned_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkCrgPHm7Z2W"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tf_idf(data):\n",
        "  concat_title_text(data)\n",
        "  combined_vectors = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=1)\n",
        "  combined_vectors.fit(data[\"cleaned_title_text\"])\n",
        "  combined_vectors_dictionary = combined_vectors.vocabulary_\n",
        "  # print (combined_vectors_dictionary)\n",
        "\n",
        "  # Now lets generate the TfIdf Vectors seperately for Title and Text from the vector dictionary\n",
        "  # built above\n",
        "  title_vectors = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=1, vocabulary=combined_vectors_dictionary)\n",
        "  title_tfidf_vectors = title_vectors.fit_transform(data['cleaned_title'])\n",
        "  print (title_tfidf_vectors.shape)\n",
        "  # print (\"\\n Title TF-IDF Vectors\")\n",
        "  # print (title_vectors.get_feature_names())     \n",
        "\n",
        "  text_vectors = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=1, vocabulary=combined_vectors_dictionary)\n",
        "  text_tfidf_vectors = text_vectors.fit_transform(data['cleaned_text'])\n",
        "\n",
        "  # print (\"\\n Text TF-IDF Vectors\")\n",
        "  # print (text_vectors.get_feature_names())\n",
        "  print (text_tfidf_vectors.shape)\n",
        "\n",
        "  return title_tfidf_vectors, text_tfidf_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox19gteECU_D"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similarity_score(data, title_vectors, text_vectors):\n",
        "  similarity_score = []\n",
        "  for i in range(len(data)):\n",
        "      similarity_score.append(1 - cosine(title_vectors[i], text_vectors[i]))\n",
        "  return similarity_score\n",
        "\n",
        "def tf_idf_similarity_score(data):\n",
        "  title_tfidf_vectors, text_tfidf_vectors = tf_idf(data)\n",
        "  data['similarity_title_text'] = similarity_score(data, title_tfidf_vectors.toarray(), text_tfidf_vectors.toarray())\n",
        "  return title_tfidf_vectors, text_tfidf_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ysINkJ9_bQ",
        "outputId": "b44f48e2-f6ee-4a51-e953-17a4711a4251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "# Tf-IDF and Cosine Similarities on Multiclass labeled dataset\n",
        "stance_title_tfidf_vectors, stance_text_tfidf_vectors = tf_idf_similarity_score(final_stance)\n",
        "final_stance.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(754, 222460)\n",
            "(754, 222460)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "      <th>title_sentences</th>\n",
              "      <th>title_neg</th>\n",
              "      <th>title_neu</th>\n",
              "      <th>title_pos</th>\n",
              "      <th>title_compound</th>\n",
              "      <th>text_sentences</th>\n",
              "      <th>text_neg</th>\n",
              "      <th>text_neu</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>text_compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sadly, Pumpkin Spice Condoms Aren't A Thing Af...</td>\n",
              "      <td>agree</td>\n",
              "      <td>Durex denies rumors of a ‘pumpkin spice’ flavo...</td>\n",
              "      <td>sad pumpkin spice condom thing</td>\n",
              "      <td>durex deni rumor pumpkin spice flavor condom o...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>[sad, pumpkin, spice, condom, thing]</td>\n",
              "      <td>[durex, deni, rumor, pumpkin, spice, flavor, c...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>[sad_pumpkin, pumpkin_spice, spice_condom, con...</td>\n",
              "      <td>[durex_deni, deni_rumor, rumor_pumpkin, pumpki...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>84</td>\n",
              "      <td>[sad_pumpkin_spice, pumpkin_spice_condom, spic...</td>\n",
              "      <td>[durex_deni_rumor, deni_rumor_pumpkin, rumor_p...</td>\n",
              "      <td>3</td>\n",
              "      <td>92</td>\n",
              "      <td>3</td>\n",
              "      <td>87</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>sad pumpkin spice condom thing durex deni rumo...</td>\n",
              "      <td>0.255239</td>\n",
              "      <td>[sadly, pumpkin, spice, condoms, aren, t, a, t...</td>\n",
              "      <td>[durex, denies, rumors, of, a, pumpkin, spice,...</td>\n",
              "      <td>0.779995</td>\n",
              "      <td>0.990667</td>\n",
              "      <td>[Sadly, Pumpkin Spice Condoms Aren't A Thing A...</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.714</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.4215</td>\n",
              "      <td>[Durex denies rumors of a ‘pumpkin spice’ flav...</td>\n",
              "      <td>0.118500</td>\n",
              "      <td>0.771900</td>\n",
              "      <td>0.109700</td>\n",
              "      <td>-0.025750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This Woman Claims To Have Had Plastic Surgery ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "      <td>woman claim plastic surgeri third breast</td>\n",
              "      <td>bereav afghan mother took reveng taliban watch...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>[woman, claim, plastic, surgeri, third, breast]</td>\n",
              "      <td>[bereav, afghan, mother, took, reveng, taliban...</td>\n",
              "      <td>6</td>\n",
              "      <td>180</td>\n",
              "      <td>6</td>\n",
              "      <td>138</td>\n",
              "      <td>[woman_claim, claim_plastic, plastic_surgeri, ...</td>\n",
              "      <td>[bereav_afghan, afghan_mother, mother_took, to...</td>\n",
              "      <td>5</td>\n",
              "      <td>179</td>\n",
              "      <td>5</td>\n",
              "      <td>175</td>\n",
              "      <td>[woman_claim_plastic, claim_plastic_surgeri, p...</td>\n",
              "      <td>[bereav_afghan_mother, afghan_mother_took, mot...</td>\n",
              "      <td>4</td>\n",
              "      <td>178</td>\n",
              "      <td>4</td>\n",
              "      <td>177</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>woman claim plastic surgeri third breast berea...</td>\n",
              "      <td>0.002978</td>\n",
              "      <td>[this, woman, claims, to, have, had, plastic, ...</td>\n",
              "      <td>[a, bereaved, afghan, mother, took, revenge, o...</td>\n",
              "      <td>0.593915</td>\n",
              "      <td>0.019286</td>\n",
              "      <td>[This Woman Claims To Have Had Plastic Surgery...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>[A bereaved Afghan mother took revenge on the ...</td>\n",
              "      <td>0.187857</td>\n",
              "      <td>0.765929</td>\n",
              "      <td>0.046286</td>\n",
              "      <td>-0.408614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Justin Bieber Saved This Guy From a Bear Attack</td>\n",
              "      <td>discuss</td>\n",
              "      <td>When faced with the choice of feasting on a fi...</td>\n",
              "      <td>justin bieber save guy bear attack</td>\n",
              "      <td>face choic feast fine meal human listen justin...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[justin, bieber, save, guy, bear, attack]</td>\n",
              "      <td>[face, choic, feast, fine, meal, human, listen...</td>\n",
              "      <td>6</td>\n",
              "      <td>83</td>\n",
              "      <td>6</td>\n",
              "      <td>76</td>\n",
              "      <td>[justin_bieber, bieber_save, save_guy, guy_bea...</td>\n",
              "      <td>[face_choic, choic_feast, feast_fine, fine_mea...</td>\n",
              "      <td>5</td>\n",
              "      <td>82</td>\n",
              "      <td>5</td>\n",
              "      <td>81</td>\n",
              "      <td>[justin_bieber_save, bieber_save_guy, save_guy...</td>\n",
              "      <td>[face_choic_feast, choic_feast_fine, feast_fin...</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>justin bieber save guy bear attack face choic ...</td>\n",
              "      <td>0.125151</td>\n",
              "      <td>[justin, bieber, saved, this, guy, from, a, be...</td>\n",
              "      <td>[when, faced, with, the, choice, of, feasting,...</td>\n",
              "      <td>0.766177</td>\n",
              "      <td>0.918633</td>\n",
              "      <td>[Justin Bieber Saved This Guy From a Bear Attack]</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.0772</td>\n",
              "      <td>[When faced with the choice of feasting on a f...</td>\n",
              "      <td>0.052167</td>\n",
              "      <td>0.856667</td>\n",
              "      <td>0.091333</td>\n",
              "      <td>0.032367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina president adopts young Jewish boy as...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ESPN host Chris Fowler was duped by a fake web...</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>espn host chris fowler dupe fake websit mediam...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[argentina, presid, adopt, young, jewish, boy,...</td>\n",
              "      <td>[espn, host, chris, fowler, dupe, fake, websit...</td>\n",
              "      <td>10</td>\n",
              "      <td>265</td>\n",
              "      <td>10</td>\n",
              "      <td>165</td>\n",
              "      <td>[argentina_presid, presid_adopt, adopt_young, ...</td>\n",
              "      <td>[espn_host, host_chris, chris_fowler, fowler_d...</td>\n",
              "      <td>9</td>\n",
              "      <td>264</td>\n",
              "      <td>9</td>\n",
              "      <td>242</td>\n",
              "      <td>[argentina_presid_adopt, presid_adopt_young, a...</td>\n",
              "      <td>[espn_host_chris, host_chris_fowler, chris_fow...</td>\n",
              "      <td>8</td>\n",
              "      <td>263</td>\n",
              "      <td>8</td>\n",
              "      <td>258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[argentina, president, adopts, young, jewish, ...</td>\n",
              "      <td>[espn, host, chris, fowler, was, duped, by, a,...</td>\n",
              "      <td>0.614044</td>\n",
              "      <td>-0.108503</td>\n",
              "      <td>[Argentina president adopts young Jewish boy a...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.2023</td>\n",
              "      <td>[ESPN host Chris Fowler was duped by a fake we...</td>\n",
              "      <td>0.050389</td>\n",
              "      <td>0.887722</td>\n",
              "      <td>0.061889</td>\n",
              "      <td>0.025100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Iraq claims ISIS leader al-Baghdadi’s wife has...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>shoot war memori parliament hill unconfirm rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[iraq, claim, isi, leader, al, baghdadi, wife,...</td>\n",
              "      <td>[shoot, war, memori, parliament, hill, unconfi...</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>[iraq_claim, claim_isi, isi_leader, leader_al,...</td>\n",
              "      <td>[shoot_war, war_memori, memori_parliament, par...</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>[iraq_claim_isi, claim_isi_leader, isi_leader_...</td>\n",
              "      <td>[shoot_war_memori, war_memori_parliament, memo...</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[iraq, claims, isis, leader, al, baghdadi, s, ...</td>\n",
              "      <td>[there, has, been, a, shooting, at, the, war, ...</td>\n",
              "      <td>0.639913</td>\n",
              "      <td>-0.037467</td>\n",
              "      <td>[Iraq claims ISIS leader al-Baghdadi’s wife ha...</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.807</td>\n",
              "      <td>0.094</td>\n",
              "      <td>-0.0377</td>\n",
              "      <td>[There has been a shooting at the War Memorial...</td>\n",
              "      <td>0.256000</td>\n",
              "      <td>0.744000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.363700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... text_compound\n",
              "0  Sadly, Pumpkin Spice Condoms Aren't A Thing Af...  ...     -0.025750\n",
              "1  This Woman Claims To Have Had Plastic Surgery ...  ...     -0.408614\n",
              "2    Justin Bieber Saved This Guy From a Bear Attack  ...      0.032367\n",
              "3  Argentina president adopts young Jewish boy as...  ...      0.025100\n",
              "4  Iraq claims ISIS leader al-Baghdadi’s wife has...  ...     -0.363700\n",
              "\n",
              "[5 rows x 44 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C39ZDz6dMoHO",
        "outputId": "ada0b1d5-dad0-4a6f-82cb-b4fd064a1441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "# Tf-IDF and Cosine Similarities on Binary class labeled dataset\n",
        "\n",
        "df_title_tfidf_vectors, df_text_tfidf_vectors = tf_idf_similarity_score(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(709, 426121)\n",
            "(709, 426121)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...</td>\n",
              "      <td>Jay Dyer 21st Century WireJamie Hanshaw, autho...</td>\n",
              "      <td>1</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch</td>\n",
              "      <td>jay dyer st centuri wirejami hanshaw author op...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[hollywood, witchcraft, dark, side, reveal, wi...</td>\n",
              "      <td>[jay, dyer, st, centuri, wirejami, hanshaw, au...</td>\n",
              "      <td>6</td>\n",
              "      <td>95</td>\n",
              "      <td>6</td>\n",
              "      <td>74</td>\n",
              "      <td>[hollywood_witchcraft, witchcraft_dark, dark_s...</td>\n",
              "      <td>[jay_dyer, dyer_st, st_centuri, centuri_wireja...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>91</td>\n",
              "      <td>[hollywood_witchcraft_dark, witchcraft_dark_si...</td>\n",
              "      <td>[jay_dyer_st, dyer_st_centuri, st_centuri_wire...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch ja...</td>\n",
              "      <td>0.079199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cyberattack on Belgian media outlets claimed b...</td>\n",
              "      <td>Mon, 24 Oct 2016 21:12 UTC © www,syrian-cyber-...</td>\n",
              "      <td>1</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>mon oct utc www syrian cyber armi net sever be...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[cyberattack, belgian, medium, outlet, claim, ...</td>\n",
              "      <td>[mon, oct, utc, www, syrian, cyber, armi, net,...</td>\n",
              "      <td>8</td>\n",
              "      <td>252</td>\n",
              "      <td>8</td>\n",
              "      <td>180</td>\n",
              "      <td>[cyberattack_belgian, belgian_medium, medium_o...</td>\n",
              "      <td>[mon_oct, oct_utc, utc_www, www_syrian, syrian...</td>\n",
              "      <td>7</td>\n",
              "      <td>251</td>\n",
              "      <td>7</td>\n",
              "      <td>240</td>\n",
              "      <td>[cyberattack_belgian_medium, belgian_medium_ou...</td>\n",
              "      <td>[mon_oct_utc, oct_utc_www, utc_www_syrian, www...</td>\n",
              "      <td>6</td>\n",
              "      <td>250</td>\n",
              "      <td>6</td>\n",
              "      <td>249</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>0.231299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>France Presidential Election 2017 Livewire</td>\n",
              "      <td>Polls have closed for the second and final rou...</td>\n",
              "      <td>0</td>\n",
              "      <td>franc presidenti elect livewir</td>\n",
              "      <td>poll close second final round vote french pres...</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>[franc, presidenti, elect, livewir]</td>\n",
              "      <td>[poll, close, second, final, round, vote, fren...</td>\n",
              "      <td>4</td>\n",
              "      <td>74</td>\n",
              "      <td>4</td>\n",
              "      <td>65</td>\n",
              "      <td>[franc_presidenti, presidenti_elect, elect_liv...</td>\n",
              "      <td>[poll_close, close_second, second_final, final...</td>\n",
              "      <td>3</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>71</td>\n",
              "      <td>[franc_presidenti_elect, presidenti_elect_live...</td>\n",
              "      <td>[poll_close_second, close_second_final, second...</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>franc presidenti elect livewir poll close seco...</td>\n",
              "      <td>0.022875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despite Republican’s Best Efforts, SCOTUS Jus...</td>\n",
              "      <td>Much to the dismay of so-called  pro-life  Rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>much dismay call pro life republican suprem co...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[despit, republican, best, effort, scotus, all...</td>\n",
              "      <td>[much, dismay, call, pro, life, republican, su...</td>\n",
              "      <td>10</td>\n",
              "      <td>210</td>\n",
              "      <td>10</td>\n",
              "      <td>133</td>\n",
              "      <td>[despit_republican, republican_best, best_effo...</td>\n",
              "      <td>[much_dismay, dismay_call, call_pro, pro_life,...</td>\n",
              "      <td>9</td>\n",
              "      <td>209</td>\n",
              "      <td>9</td>\n",
              "      <td>194</td>\n",
              "      <td>[despit_republican_best, republican_best_effor...</td>\n",
              "      <td>[much_dismay_call, dismay_call_pro, call_pro_l...</td>\n",
              "      <td>8</td>\n",
              "      <td>208</td>\n",
              "      <td>8</td>\n",
              "      <td>206</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>0.178999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNC Chair’s Comment Reveals Why Republicans M...</td>\n",
              "      <td>You almost have to feel sorry for Republicans ...</td>\n",
              "      <td>1</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>almost feel sorri republican almost find reall...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[rnc, chair, comment, reveal, republican, may,...</td>\n",
              "      <td>[almost, feel, sorri, republican, almost, find...</td>\n",
              "      <td>9</td>\n",
              "      <td>289</td>\n",
              "      <td>9</td>\n",
              "      <td>210</td>\n",
              "      <td>[rnc_chair, chair_comment, comment_reveal, rev...</td>\n",
              "      <td>[almost_feel, feel_sorri, sorri_republican, re...</td>\n",
              "      <td>8</td>\n",
              "      <td>288</td>\n",
              "      <td>8</td>\n",
              "      <td>282</td>\n",
              "      <td>[rnc_chair_comment, chair_comment_reveal, comm...</td>\n",
              "      <td>[almost_feel_sorri, feel_sorri_republican, sor...</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>0.040344</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... similarity_title_text\n",
              "0  HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...  ...              0.079199\n",
              "1  Cyberattack on Belgian media outlets claimed b...  ...              0.231299\n",
              "2         France Presidential Election 2017 Livewire  ...              0.022875\n",
              "3   Despite Republican’s Best Efforts, SCOTUS Jus...  ...              0.178999\n",
              "4   RNC Chair’s Comment Reveals Why Republicans M...  ...              0.040344\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "213tPwgLUw1p"
      },
      "source": [
        "## Word2Vec using Google Corpus and Similarity scores\n",
        "\n",
        "Now we will generate the word vectors for title and text by training each of the unigram words on Google News corpus. This is used to find synonyms for a word, which will then be used to calculate the similaries scores between the synonyms rather than the exact words.\n",
        "\n",
        "To use Word2Vec, we need title and text with out lemmatization and stop words performed on it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgNHgN0RVgsX"
      },
      "source": [
        "def cleaning_simple_tokenize(raw_news):\n",
        "    import nltk\n",
        "    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    return np.array(news_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5LHCL0iXoeq",
        "outputId": "f766b43d-a600-4ec0-ca5e-c51ad7db2c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import gensim\n",
        "\n",
        "google_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/Shared drives/SheCodes/Datasets/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srzLEcBepWny"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "standard_scaler = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JK4cuqocrba"
      },
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "import functools \n",
        "\n",
        "def sum_word2vec_scores(words):\n",
        "  scores = []\n",
        "  for word in words:\n",
        "    if (word in google_model):\n",
        "      scores.append(np.sum(google_model[word]))\n",
        "\n",
        "\n",
        "    \n",
        "  # a = [x + y for x, y in zip(arr, [0.] * 300)]\n",
        "  zeros_arr = np.array([0.] * 300)\n",
        "  print(300 - len(scores))\n",
        "  scores = np.pad(scores, (0, 300 - len(scores)), 'constant')\n",
        "  a = np.add(scores, zeros_arr)\n",
        "  print (len(a))\n",
        "  return a\n",
        "  \n",
        "\n",
        "def word2vec(data, title_column, text_column):\n",
        "  # title_word2vec = data[title_column].map(lambda x: sum_word2vec_scores(x))\n",
        "  text_word2vec = data[text_column].map(lambda x: sum_word2vec_scores(x))\n",
        "  return title_word2vec, text_word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK_i7l0_AwSt"
      },
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from functools import reduce\n",
        "\n",
        "def word2vec_similarity_score(data):\n",
        "  data['word2vec_cleaned_title'] = data[\"title\"].map(lambda x: cleaning_simple_tokenize(x))\n",
        "  data['word2vec_cleaned_text'] = data[\"text\"].map(lambda x: cleaning_simple_tokenize(x))\n",
        "\n",
        "  title_word2vec = data['word2vec_cleaned_title'].map(lambda words: reduce(np.add, [google_model[word] for word in words if word in google_model], [0.]*300))\n",
        "  text_word2vec = data['word2vec_cleaned_text'].map(lambda words: reduce(np.add, [google_model[word] for word in words if word in google_model], [0.]*300))\n",
        "\n",
        "  data['word2vec_similarity_title_text'] = similarity_score(data, np.array(title_word2vec), np.array(text_word2vec))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRT6nREKrcYR",
        "outputId": "f90df807-d7b6-4eef-b542-22a31c98be19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "# Word2Vec and Cosine Similarities on Multiclass labeled dataset\n",
        "\n",
        "word2vec_similarity_score(final_stance)\n",
        "final_stance.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sadly, Pumpkin Spice Condoms Aren't A Thing Af...</td>\n",
              "      <td>agree</td>\n",
              "      <td>Durex denies rumors of a ‘pumpkin spice’ flavo...</td>\n",
              "      <td>sad pumpkin spice condom thing</td>\n",
              "      <td>durex deni rumor pumpkin spice flavor condom o...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>[sad, pumpkin, spice, condom, thing]</td>\n",
              "      <td>[durex, deni, rumor, pumpkin, spice, flavor, c...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>[sad_pumpkin, pumpkin_spice, spice_condom, con...</td>\n",
              "      <td>[durex_deni, deni_rumor, rumor_pumpkin, pumpki...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>84</td>\n",
              "      <td>[sad_pumpkin_spice, pumpkin_spice_condom, spic...</td>\n",
              "      <td>[durex_deni_rumor, deni_rumor_pumpkin, rumor_p...</td>\n",
              "      <td>3</td>\n",
              "      <td>92</td>\n",
              "      <td>3</td>\n",
              "      <td>87</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>sad pumpkin spice condom thing durex deni rumo...</td>\n",
              "      <td>0.255239</td>\n",
              "      <td>[sadly, pumpkin, spice, condoms, aren, t, a, t...</td>\n",
              "      <td>[durex, denies, rumors, of, a, pumpkin, spice,...</td>\n",
              "      <td>0.779995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This Woman Claims To Have Had Plastic Surgery ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "      <td>woman claim plastic surgeri third breast</td>\n",
              "      <td>bereav afghan mother took reveng taliban watch...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>[woman, claim, plastic, surgeri, third, breast]</td>\n",
              "      <td>[bereav, afghan, mother, took, reveng, taliban...</td>\n",
              "      <td>6</td>\n",
              "      <td>180</td>\n",
              "      <td>6</td>\n",
              "      <td>138</td>\n",
              "      <td>[woman_claim, claim_plastic, plastic_surgeri, ...</td>\n",
              "      <td>[bereav_afghan, afghan_mother, mother_took, to...</td>\n",
              "      <td>5</td>\n",
              "      <td>179</td>\n",
              "      <td>5</td>\n",
              "      <td>175</td>\n",
              "      <td>[woman_claim_plastic, claim_plastic_surgeri, p...</td>\n",
              "      <td>[bereav_afghan_mother, afghan_mother_took, mot...</td>\n",
              "      <td>4</td>\n",
              "      <td>178</td>\n",
              "      <td>4</td>\n",
              "      <td>177</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>woman claim plastic surgeri third breast berea...</td>\n",
              "      <td>0.002978</td>\n",
              "      <td>[this, woman, claims, to, have, had, plastic, ...</td>\n",
              "      <td>[a, bereaved, afghan, mother, took, revenge, o...</td>\n",
              "      <td>0.593915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Justin Bieber Saved This Guy From a Bear Attack</td>\n",
              "      <td>discuss</td>\n",
              "      <td>When faced with the choice of feasting on a fi...</td>\n",
              "      <td>justin bieber save guy bear attack</td>\n",
              "      <td>face choic feast fine meal human listen justin...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[justin, bieber, save, guy, bear, attack]</td>\n",
              "      <td>[face, choic, feast, fine, meal, human, listen...</td>\n",
              "      <td>6</td>\n",
              "      <td>83</td>\n",
              "      <td>6</td>\n",
              "      <td>76</td>\n",
              "      <td>[justin_bieber, bieber_save, save_guy, guy_bea...</td>\n",
              "      <td>[face_choic, choic_feast, feast_fine, fine_mea...</td>\n",
              "      <td>5</td>\n",
              "      <td>82</td>\n",
              "      <td>5</td>\n",
              "      <td>81</td>\n",
              "      <td>[justin_bieber_save, bieber_save_guy, save_guy...</td>\n",
              "      <td>[face_choic_feast, choic_feast_fine, feast_fin...</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>justin bieber save guy bear attack face choic ...</td>\n",
              "      <td>0.125151</td>\n",
              "      <td>[justin, bieber, saved, this, guy, from, a, be...</td>\n",
              "      <td>[when, faced, with, the, choice, of, feasting,...</td>\n",
              "      <td>0.766177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina president adopts young Jewish boy as...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ESPN host Chris Fowler was duped by a fake web...</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>espn host chris fowler dupe fake websit mediam...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[argentina, presid, adopt, young, jewish, boy,...</td>\n",
              "      <td>[espn, host, chris, fowler, dupe, fake, websit...</td>\n",
              "      <td>10</td>\n",
              "      <td>265</td>\n",
              "      <td>10</td>\n",
              "      <td>165</td>\n",
              "      <td>[argentina_presid, presid_adopt, adopt_young, ...</td>\n",
              "      <td>[espn_host, host_chris, chris_fowler, fowler_d...</td>\n",
              "      <td>9</td>\n",
              "      <td>264</td>\n",
              "      <td>9</td>\n",
              "      <td>242</td>\n",
              "      <td>[argentina_presid_adopt, presid_adopt_young, a...</td>\n",
              "      <td>[espn_host_chris, host_chris_fowler, chris_fow...</td>\n",
              "      <td>8</td>\n",
              "      <td>263</td>\n",
              "      <td>8</td>\n",
              "      <td>258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[argentina, president, adopts, young, jewish, ...</td>\n",
              "      <td>[espn, host, chris, fowler, was, duped, by, a,...</td>\n",
              "      <td>0.614044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Iraq claims ISIS leader al-Baghdadi’s wife has...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>shoot war memori parliament hill unconfirm rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[iraq, claim, isi, leader, al, baghdadi, wife,...</td>\n",
              "      <td>[shoot, war, memori, parliament, hill, unconfi...</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>[iraq_claim, claim_isi, isi_leader, leader_al,...</td>\n",
              "      <td>[shoot_war, war_memori, memori_parliament, par...</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>[iraq_claim_isi, claim_isi_leader, isi_leader_...</td>\n",
              "      <td>[shoot_war_memori, war_memori_parliament, memo...</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[iraq, claims, isis, leader, al, baghdadi, s, ...</td>\n",
              "      <td>[there, has, been, a, shooting, at, the, war, ...</td>\n",
              "      <td>0.639913</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... word2vec_similarity_title_text\n",
              "0  Sadly, Pumpkin Spice Condoms Aren't A Thing Af...  ...                       0.779995\n",
              "1  This Woman Claims To Have Had Plastic Surgery ...  ...                       0.593915\n",
              "2    Justin Bieber Saved This Guy From a Bear Attack  ...                       0.766177\n",
              "3  Argentina president adopts young Jewish boy as...  ...                       0.614044\n",
              "4  Iraq claims ISIS leader al-Baghdadi’s wife has...  ...                       0.639913\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk7QoTQwNx3F",
        "outputId": "600e5e44-0b8d-422f-a372-9ad4f7360981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Word2Vec and Cosine Similarities on Binary Class labeled dataset\n",
        "\n",
        "word2vec_similarity_score(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...</td>\n",
              "      <td>Jay Dyer 21st Century WireJamie Hanshaw, autho...</td>\n",
              "      <td>1</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch</td>\n",
              "      <td>jay dyer st centuri wirejami hanshaw author op...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[hollywood, witchcraft, dark, side, reveal, wi...</td>\n",
              "      <td>[jay, dyer, st, centuri, wirejami, hanshaw, au...</td>\n",
              "      <td>6</td>\n",
              "      <td>95</td>\n",
              "      <td>6</td>\n",
              "      <td>74</td>\n",
              "      <td>[hollywood_witchcraft, witchcraft_dark, dark_s...</td>\n",
              "      <td>[jay_dyer, dyer_st, st_centuri, centuri_wireja...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>91</td>\n",
              "      <td>[hollywood_witchcraft_dark, witchcraft_dark_si...</td>\n",
              "      <td>[jay_dyer_st, dyer_st_centuri, st_centuri_wire...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch ja...</td>\n",
              "      <td>0.079199</td>\n",
              "      <td>[hollywood, witchcraft, the, dark, side, revea...</td>\n",
              "      <td>[jay, dyer, st, century, wirejamie, hanshaw, a...</td>\n",
              "      <td>0.704981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cyberattack on Belgian media outlets claimed b...</td>\n",
              "      <td>Mon, 24 Oct 2016 21:12 UTC © www,syrian-cyber-...</td>\n",
              "      <td>1</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>mon oct utc www syrian cyber armi net sever be...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[cyberattack, belgian, medium, outlet, claim, ...</td>\n",
              "      <td>[mon, oct, utc, www, syrian, cyber, armi, net,...</td>\n",
              "      <td>8</td>\n",
              "      <td>252</td>\n",
              "      <td>8</td>\n",
              "      <td>180</td>\n",
              "      <td>[cyberattack_belgian, belgian_medium, medium_o...</td>\n",
              "      <td>[mon_oct, oct_utc, utc_www, www_syrian, syrian...</td>\n",
              "      <td>7</td>\n",
              "      <td>251</td>\n",
              "      <td>7</td>\n",
              "      <td>240</td>\n",
              "      <td>[cyberattack_belgian_medium, belgian_medium_ou...</td>\n",
              "      <td>[mon_oct_utc, oct_utc_www, utc_www_syrian, www...</td>\n",
              "      <td>6</td>\n",
              "      <td>250</td>\n",
              "      <td>6</td>\n",
              "      <td>249</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>0.231299</td>\n",
              "      <td>[cyberattack, on, belgian, media, outlets, cla...</td>\n",
              "      <td>[mon, oct, utc, www, syrian, cyber, army, net,...</td>\n",
              "      <td>0.708456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>France Presidential Election 2017 Livewire</td>\n",
              "      <td>Polls have closed for the second and final rou...</td>\n",
              "      <td>0</td>\n",
              "      <td>franc presidenti elect livewir</td>\n",
              "      <td>poll close second final round vote french pres...</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>[franc, presidenti, elect, livewir]</td>\n",
              "      <td>[poll, close, second, final, round, vote, fren...</td>\n",
              "      <td>4</td>\n",
              "      <td>74</td>\n",
              "      <td>4</td>\n",
              "      <td>65</td>\n",
              "      <td>[franc_presidenti, presidenti_elect, elect_liv...</td>\n",
              "      <td>[poll_close, close_second, second_final, final...</td>\n",
              "      <td>3</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>71</td>\n",
              "      <td>[franc_presidenti_elect, presidenti_elect_live...</td>\n",
              "      <td>[poll_close_second, close_second_final, second...</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>franc presidenti elect livewir poll close seco...</td>\n",
              "      <td>0.022875</td>\n",
              "      <td>[france, presidential, election, livewire]</td>\n",
              "      <td>[polls, have, closed, for, the, second, and, f...</td>\n",
              "      <td>0.471907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despite Republican’s Best Efforts, SCOTUS Jus...</td>\n",
              "      <td>Much to the dismay of so-called  pro-life  Rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>much dismay call pro life republican suprem co...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[despit, republican, best, effort, scotus, all...</td>\n",
              "      <td>[much, dismay, call, pro, life, republican, su...</td>\n",
              "      <td>10</td>\n",
              "      <td>210</td>\n",
              "      <td>10</td>\n",
              "      <td>133</td>\n",
              "      <td>[despit_republican, republican_best, best_effo...</td>\n",
              "      <td>[much_dismay, dismay_call, call_pro, pro_life,...</td>\n",
              "      <td>9</td>\n",
              "      <td>209</td>\n",
              "      <td>9</td>\n",
              "      <td>194</td>\n",
              "      <td>[despit_republican_best, republican_best_effor...</td>\n",
              "      <td>[much_dismay_call, dismay_call_pro, call_pro_l...</td>\n",
              "      <td>8</td>\n",
              "      <td>208</td>\n",
              "      <td>8</td>\n",
              "      <td>206</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>0.178999</td>\n",
              "      <td>[despite, republican, s, best, efforts, scotus...</td>\n",
              "      <td>[much, to, the, dismay, of, so, called, pro, l...</td>\n",
              "      <td>0.733165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNC Chair’s Comment Reveals Why Republicans M...</td>\n",
              "      <td>You almost have to feel sorry for Republicans ...</td>\n",
              "      <td>1</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>almost feel sorri republican almost find reall...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[rnc, chair, comment, reveal, republican, may,...</td>\n",
              "      <td>[almost, feel, sorri, republican, almost, find...</td>\n",
              "      <td>9</td>\n",
              "      <td>289</td>\n",
              "      <td>9</td>\n",
              "      <td>210</td>\n",
              "      <td>[rnc_chair, chair_comment, comment_reveal, rev...</td>\n",
              "      <td>[almost_feel, feel_sorri, sorri_republican, re...</td>\n",
              "      <td>8</td>\n",
              "      <td>288</td>\n",
              "      <td>8</td>\n",
              "      <td>282</td>\n",
              "      <td>[rnc_chair_comment, chair_comment_reveal, comm...</td>\n",
              "      <td>[almost_feel_sorri, feel_sorri_republican, sor...</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>0.040344</td>\n",
              "      <td>[rnc, chair, s, comment, reveals, why, republi...</td>\n",
              "      <td>[you, almost, have, to, feel, sorry, for, repu...</td>\n",
              "      <td>0.764235</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... word2vec_similarity_title_text\n",
              "0  HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...  ...                       0.704981\n",
              "1  Cyberattack on Belgian media outlets claimed b...  ...                       0.708456\n",
              "2         France Presidential Election 2017 Livewire  ...                       0.471907\n",
              "3   Despite Republican’s Best Efforts, SCOTUS Jus...  ...                       0.733165\n",
              "4   RNC Chair’s Comment Reveals Why Republicans M...  ...                       0.764235\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05VwWbtpGR44"
      },
      "source": [
        "## LSA Topic Modelling using SVD and Cosine Similarities\n",
        "\n",
        "Lets now try to find topics from the title and text using SVD. Then find the cosine similarity between the generated title and text topics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxO043YTGQ7O"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "def svd(data, title_tfidf_vectors, text_tfidf_vectors):\n",
        "  truncated_svd = TruncatedSVD(n_components=100, n_iter=10)\n",
        "  combined_vectors = vstack([title_tfidf_vectors, text_tfidf_vectors])\n",
        "  truncated_svd.fit(combined_vectors)\n",
        "\n",
        "  title_svd = truncated_svd.transform(title_tfidf_vectors)\n",
        "  print (title_svd.shape)\n",
        "  text_svd = truncated_svd.transform(text_tfidf_vectors)\n",
        "  print (text_svd.shape)\n",
        "  return title_svd, text_svd\n",
        "\n",
        "def topic_similarity_score(data, title_tfidf_vectors, text_tfidf_vectors):\n",
        "  title_svd_vectors, text_svd_vectors = svd(data, title_tfidf_vectors, text_tfidf_vectors)\n",
        "  data['topics_similarity_title_text'] = similarity_score(data, title_svd_vectors, text_svd_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPszGIt5HqnW",
        "outputId": "cc198b97-5acc-42a2-9771-42be1cb49a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "# LSA Topic Modelling and Cosine Similarities on Multi Class labeled dataset\n",
        "topic_similarity_score(final_stance, stance_title_tfidf_vectors, stance_text_tfidf_vectors)\n",
        "final_stance.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(754, 100)\n",
            "(754, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sadly, Pumpkin Spice Condoms Aren't A Thing Af...</td>\n",
              "      <td>agree</td>\n",
              "      <td>Durex denies rumors of a ‘pumpkin spice’ flavo...</td>\n",
              "      <td>sad pumpkin spice condom thing</td>\n",
              "      <td>durex deni rumor pumpkin spice flavor condom o...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>[sad, pumpkin, spice, condom, thing]</td>\n",
              "      <td>[durex, deni, rumor, pumpkin, spice, flavor, c...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>[sad_pumpkin, pumpkin_spice, spice_condom, con...</td>\n",
              "      <td>[durex_deni, deni_rumor, rumor_pumpkin, pumpki...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>84</td>\n",
              "      <td>[sad_pumpkin_spice, pumpkin_spice_condom, spic...</td>\n",
              "      <td>[durex_deni_rumor, deni_rumor_pumpkin, rumor_p...</td>\n",
              "      <td>3</td>\n",
              "      <td>92</td>\n",
              "      <td>3</td>\n",
              "      <td>87</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>sad pumpkin spice condom thing durex deni rumo...</td>\n",
              "      <td>0.255239</td>\n",
              "      <td>[sadly, pumpkin, spice, condoms, aren, t, a, t...</td>\n",
              "      <td>[durex, denies, rumors, of, a, pumpkin, spice,...</td>\n",
              "      <td>0.779995</td>\n",
              "      <td>0.990667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This Woman Claims To Have Had Plastic Surgery ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "      <td>woman claim plastic surgeri third breast</td>\n",
              "      <td>bereav afghan mother took reveng taliban watch...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>[woman, claim, plastic, surgeri, third, breast]</td>\n",
              "      <td>[bereav, afghan, mother, took, reveng, taliban...</td>\n",
              "      <td>6</td>\n",
              "      <td>180</td>\n",
              "      <td>6</td>\n",
              "      <td>138</td>\n",
              "      <td>[woman_claim, claim_plastic, plastic_surgeri, ...</td>\n",
              "      <td>[bereav_afghan, afghan_mother, mother_took, to...</td>\n",
              "      <td>5</td>\n",
              "      <td>179</td>\n",
              "      <td>5</td>\n",
              "      <td>175</td>\n",
              "      <td>[woman_claim_plastic, claim_plastic_surgeri, p...</td>\n",
              "      <td>[bereav_afghan_mother, afghan_mother_took, mot...</td>\n",
              "      <td>4</td>\n",
              "      <td>178</td>\n",
              "      <td>4</td>\n",
              "      <td>177</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>woman claim plastic surgeri third breast berea...</td>\n",
              "      <td>0.002978</td>\n",
              "      <td>[this, woman, claims, to, have, had, plastic, ...</td>\n",
              "      <td>[a, bereaved, afghan, mother, took, revenge, o...</td>\n",
              "      <td>0.593915</td>\n",
              "      <td>0.019286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Justin Bieber Saved This Guy From a Bear Attack</td>\n",
              "      <td>discuss</td>\n",
              "      <td>When faced with the choice of feasting on a fi...</td>\n",
              "      <td>justin bieber save guy bear attack</td>\n",
              "      <td>face choic feast fine meal human listen justin...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[justin, bieber, save, guy, bear, attack]</td>\n",
              "      <td>[face, choic, feast, fine, meal, human, listen...</td>\n",
              "      <td>6</td>\n",
              "      <td>83</td>\n",
              "      <td>6</td>\n",
              "      <td>76</td>\n",
              "      <td>[justin_bieber, bieber_save, save_guy, guy_bea...</td>\n",
              "      <td>[face_choic, choic_feast, feast_fine, fine_mea...</td>\n",
              "      <td>5</td>\n",
              "      <td>82</td>\n",
              "      <td>5</td>\n",
              "      <td>81</td>\n",
              "      <td>[justin_bieber_save, bieber_save_guy, save_guy...</td>\n",
              "      <td>[face_choic_feast, choic_feast_fine, feast_fin...</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>justin bieber save guy bear attack face choic ...</td>\n",
              "      <td>0.125151</td>\n",
              "      <td>[justin, bieber, saved, this, guy, from, a, be...</td>\n",
              "      <td>[when, faced, with, the, choice, of, feasting,...</td>\n",
              "      <td>0.766177</td>\n",
              "      <td>0.918633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina president adopts young Jewish boy as...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ESPN host Chris Fowler was duped by a fake web...</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>espn host chris fowler dupe fake websit mediam...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[argentina, presid, adopt, young, jewish, boy,...</td>\n",
              "      <td>[espn, host, chris, fowler, dupe, fake, websit...</td>\n",
              "      <td>10</td>\n",
              "      <td>265</td>\n",
              "      <td>10</td>\n",
              "      <td>165</td>\n",
              "      <td>[argentina_presid, presid_adopt, adopt_young, ...</td>\n",
              "      <td>[espn_host, host_chris, chris_fowler, fowler_d...</td>\n",
              "      <td>9</td>\n",
              "      <td>264</td>\n",
              "      <td>9</td>\n",
              "      <td>242</td>\n",
              "      <td>[argentina_presid_adopt, presid_adopt_young, a...</td>\n",
              "      <td>[espn_host_chris, host_chris_fowler, chris_fow...</td>\n",
              "      <td>8</td>\n",
              "      <td>263</td>\n",
              "      <td>8</td>\n",
              "      <td>258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[argentina, president, adopts, young, jewish, ...</td>\n",
              "      <td>[espn, host, chris, fowler, was, duped, by, a,...</td>\n",
              "      <td>0.614044</td>\n",
              "      <td>-0.108503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Iraq claims ISIS leader al-Baghdadi’s wife has...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>shoot war memori parliament hill unconfirm rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[iraq, claim, isi, leader, al, baghdadi, wife,...</td>\n",
              "      <td>[shoot, war, memori, parliament, hill, unconfi...</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>[iraq_claim, claim_isi, isi_leader, leader_al,...</td>\n",
              "      <td>[shoot_war, war_memori, memori_parliament, par...</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>[iraq_claim_isi, claim_isi_leader, isi_leader_...</td>\n",
              "      <td>[shoot_war_memori, war_memori_parliament, memo...</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[iraq, claims, isis, leader, al, baghdadi, s, ...</td>\n",
              "      <td>[there, has, been, a, shooting, at, the, war, ...</td>\n",
              "      <td>0.639913</td>\n",
              "      <td>-0.037467</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... topics_similarity_title_text\n",
              "0  Sadly, Pumpkin Spice Condoms Aren't A Thing Af...  ...                     0.990667\n",
              "1  This Woman Claims To Have Had Plastic Surgery ...  ...                     0.019286\n",
              "2    Justin Bieber Saved This Guy From a Bear Attack  ...                     0.918633\n",
              "3  Argentina president adopts young Jewish boy as...  ...                    -0.108503\n",
              "4  Iraq claims ISIS leader al-Baghdadi’s wife has...  ...                    -0.037467\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoLwfmnJON-w",
        "outputId": "0fc73228-5089-4897-ec99-8d8ab6050050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "# LSA Topic Modelling and Cosine Similarities on Binary Class labeled dataset\n",
        "\n",
        "topic_similarity_score(df_final, df_title_tfidf_vectors, df_text_tfidf_vectors)\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(709, 100)\n",
            "(709, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...</td>\n",
              "      <td>Jay Dyer 21st Century WireJamie Hanshaw, autho...</td>\n",
              "      <td>1</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch</td>\n",
              "      <td>jay dyer st centuri wirejami hanshaw author op...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[hollywood, witchcraft, dark, side, reveal, wi...</td>\n",
              "      <td>[jay, dyer, st, centuri, wirejami, hanshaw, au...</td>\n",
              "      <td>6</td>\n",
              "      <td>95</td>\n",
              "      <td>6</td>\n",
              "      <td>74</td>\n",
              "      <td>[hollywood_witchcraft, witchcraft_dark, dark_s...</td>\n",
              "      <td>[jay_dyer, dyer_st, st_centuri, centuri_wireja...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>91</td>\n",
              "      <td>[hollywood_witchcraft_dark, witchcraft_dark_si...</td>\n",
              "      <td>[jay_dyer_st, dyer_st_centuri, st_centuri_wire...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch ja...</td>\n",
              "      <td>0.079199</td>\n",
              "      <td>[hollywood, witchcraft, the, dark, side, revea...</td>\n",
              "      <td>[jay, dyer, st, century, wirejamie, hanshaw, a...</td>\n",
              "      <td>0.704981</td>\n",
              "      <td>0.792957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cyberattack on Belgian media outlets claimed b...</td>\n",
              "      <td>Mon, 24 Oct 2016 21:12 UTC © www,syrian-cyber-...</td>\n",
              "      <td>1</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>mon oct utc www syrian cyber armi net sever be...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[cyberattack, belgian, medium, outlet, claim, ...</td>\n",
              "      <td>[mon, oct, utc, www, syrian, cyber, armi, net,...</td>\n",
              "      <td>8</td>\n",
              "      <td>252</td>\n",
              "      <td>8</td>\n",
              "      <td>180</td>\n",
              "      <td>[cyberattack_belgian, belgian_medium, medium_o...</td>\n",
              "      <td>[mon_oct, oct_utc, utc_www, www_syrian, syrian...</td>\n",
              "      <td>7</td>\n",
              "      <td>251</td>\n",
              "      <td>7</td>\n",
              "      <td>240</td>\n",
              "      <td>[cyberattack_belgian_medium, belgian_medium_ou...</td>\n",
              "      <td>[mon_oct_utc, oct_utc_www, utc_www_syrian, www...</td>\n",
              "      <td>6</td>\n",
              "      <td>250</td>\n",
              "      <td>6</td>\n",
              "      <td>249</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>0.231299</td>\n",
              "      <td>[cyberattack, on, belgian, media, outlets, cla...</td>\n",
              "      <td>[mon, oct, utc, www, syrian, cyber, army, net,...</td>\n",
              "      <td>0.708456</td>\n",
              "      <td>0.816659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>France Presidential Election 2017 Livewire</td>\n",
              "      <td>Polls have closed for the second and final rou...</td>\n",
              "      <td>0</td>\n",
              "      <td>franc presidenti elect livewir</td>\n",
              "      <td>poll close second final round vote french pres...</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>[franc, presidenti, elect, livewir]</td>\n",
              "      <td>[poll, close, second, final, round, vote, fren...</td>\n",
              "      <td>4</td>\n",
              "      <td>74</td>\n",
              "      <td>4</td>\n",
              "      <td>65</td>\n",
              "      <td>[franc_presidenti, presidenti_elect, elect_liv...</td>\n",
              "      <td>[poll_close, close_second, second_final, final...</td>\n",
              "      <td>3</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>71</td>\n",
              "      <td>[franc_presidenti_elect, presidenti_elect_live...</td>\n",
              "      <td>[poll_close_second, close_second_final, second...</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>franc presidenti elect livewir poll close seco...</td>\n",
              "      <td>0.022875</td>\n",
              "      <td>[france, presidential, election, livewire]</td>\n",
              "      <td>[polls, have, closed, for, the, second, and, f...</td>\n",
              "      <td>0.471907</td>\n",
              "      <td>0.203756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despite Republican’s Best Efforts, SCOTUS Jus...</td>\n",
              "      <td>Much to the dismay of so-called  pro-life  Rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>much dismay call pro life republican suprem co...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[despit, republican, best, effort, scotus, all...</td>\n",
              "      <td>[much, dismay, call, pro, life, republican, su...</td>\n",
              "      <td>10</td>\n",
              "      <td>210</td>\n",
              "      <td>10</td>\n",
              "      <td>133</td>\n",
              "      <td>[despit_republican, republican_best, best_effo...</td>\n",
              "      <td>[much_dismay, dismay_call, call_pro, pro_life,...</td>\n",
              "      <td>9</td>\n",
              "      <td>209</td>\n",
              "      <td>9</td>\n",
              "      <td>194</td>\n",
              "      <td>[despit_republican_best, republican_best_effor...</td>\n",
              "      <td>[much_dismay_call, dismay_call_pro, call_pro_l...</td>\n",
              "      <td>8</td>\n",
              "      <td>208</td>\n",
              "      <td>8</td>\n",
              "      <td>206</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>0.178999</td>\n",
              "      <td>[despite, republican, s, best, efforts, scotus...</td>\n",
              "      <td>[much, to, the, dismay, of, so, called, pro, l...</td>\n",
              "      <td>0.733165</td>\n",
              "      <td>0.719739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNC Chair’s Comment Reveals Why Republicans M...</td>\n",
              "      <td>You almost have to feel sorry for Republicans ...</td>\n",
              "      <td>1</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>almost feel sorri republican almost find reall...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[rnc, chair, comment, reveal, republican, may,...</td>\n",
              "      <td>[almost, feel, sorri, republican, almost, find...</td>\n",
              "      <td>9</td>\n",
              "      <td>289</td>\n",
              "      <td>9</td>\n",
              "      <td>210</td>\n",
              "      <td>[rnc_chair, chair_comment, comment_reveal, rev...</td>\n",
              "      <td>[almost_feel, feel_sorri, sorri_republican, re...</td>\n",
              "      <td>8</td>\n",
              "      <td>288</td>\n",
              "      <td>8</td>\n",
              "      <td>282</td>\n",
              "      <td>[rnc_chair_comment, chair_comment_reveal, comm...</td>\n",
              "      <td>[almost_feel_sorri, feel_sorri_republican, sor...</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>0.040344</td>\n",
              "      <td>[rnc, chair, s, comment, reveals, why, republi...</td>\n",
              "      <td>[you, almost, have, to, feel, sorry, for, repu...</td>\n",
              "      <td>0.764235</td>\n",
              "      <td>0.472047</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... topics_similarity_title_text\n",
              "0  HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...  ...                     0.792957\n",
              "1  Cyberattack on Belgian media outlets claimed b...  ...                     0.816659\n",
              "2         France Presidential Election 2017 Livewire  ...                     0.203756\n",
              "3   Despite Republican’s Best Efforts, SCOTUS Jus...  ...                     0.719739\n",
              "4   RNC Chair’s Comment Reveals Why Republicans M...  ...                     0.472047\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNDRqdAxvQIZ"
      },
      "source": [
        "## Sentiment Analysis and Cosine Similaties\n",
        "\n",
        "Now lets assign polarity sentiment score to the title and text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cP2_j1yvoIJ"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def compute_sentiment(sentences):\n",
        "    result = []\n",
        "    for sentence in sentences:\n",
        "        vs = sentiment_analyzer.polarity_scores(sentence)\n",
        "        result.append(vs)\n",
        "    return pd.DataFrame(result).mean()\n",
        "\n",
        "def title_sentiment(data):\n",
        "  data['title_sentences'] = data['title'].apply(lambda x: sent_tokenize(x))\n",
        "  data = pd.concat([data, data['title_sentences'].apply(lambda x: compute_sentiment(x))], axis=1)\n",
        "  data.rename(columns={'compound':'title_compound', 'neg':'title_neg', 'neu':'title_neu', 'pos':'title_pos'}, inplace=True)\n",
        "  return data\n",
        "\n",
        "\n",
        "def text_sentiment(data):\n",
        "  data['text_sentences'] = data['text'].apply(lambda x: sent_tokenize(x))\n",
        "  data = pd.concat([data, data['text_sentences'].apply(lambda x: compute_sentiment(x))], axis=1)\n",
        "  data.rename(columns={'compound':'text_compound', 'neg':'text_neg', 'neu':'text_neu', 'pos':'text_pos'}, inplace=True)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAgdY_Dkwx79",
        "outputId": "8f2cc548-4706-4242-f276-1b32a5c504f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "# Sentiment Analysis on Multi Class labeled dataset\n",
        "\n",
        "final_stance = title_sentiment(final_stance)\n",
        "final_stance = text_sentiment(final_stance)\n",
        "final_stance.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "      <th>title_sentences</th>\n",
              "      <th>title_neg</th>\n",
              "      <th>title_neu</th>\n",
              "      <th>title_pos</th>\n",
              "      <th>title_compound</th>\n",
              "      <th>text_sentences</th>\n",
              "      <th>text_neg</th>\n",
              "      <th>text_neu</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>text_compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sadly, Pumpkin Spice Condoms Aren't A Thing Af...</td>\n",
              "      <td>agree</td>\n",
              "      <td>Durex denies rumors of a ‘pumpkin spice’ flavo...</td>\n",
              "      <td>sad pumpkin spice condom thing</td>\n",
              "      <td>durex deni rumor pumpkin spice flavor condom o...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>[sad, pumpkin, spice, condom, thing]</td>\n",
              "      <td>[durex, deni, rumor, pumpkin, spice, flavor, c...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>[sad_pumpkin, pumpkin_spice, spice_condom, con...</td>\n",
              "      <td>[durex_deni, deni_rumor, rumor_pumpkin, pumpki...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>84</td>\n",
              "      <td>[sad_pumpkin_spice, pumpkin_spice_condom, spic...</td>\n",
              "      <td>[durex_deni_rumor, deni_rumor_pumpkin, rumor_p...</td>\n",
              "      <td>3</td>\n",
              "      <td>92</td>\n",
              "      <td>3</td>\n",
              "      <td>87</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>sad pumpkin spice condom thing durex deni rumo...</td>\n",
              "      <td>0.255239</td>\n",
              "      <td>[sadly, pumpkin, spice, condoms, aren, t, a, t...</td>\n",
              "      <td>[durex, denies, rumors, of, a, pumpkin, spice,...</td>\n",
              "      <td>0.779995</td>\n",
              "      <td>0.990667</td>\n",
              "      <td>[Sadly, Pumpkin Spice Condoms Aren't A Thing A...</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.714</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.4215</td>\n",
              "      <td>[Durex denies rumors of a ‘pumpkin spice’ flav...</td>\n",
              "      <td>0.118500</td>\n",
              "      <td>0.771900</td>\n",
              "      <td>0.109700</td>\n",
              "      <td>-0.025750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This Woman Claims To Have Had Plastic Surgery ...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
              "      <td>woman claim plastic surgeri third breast</td>\n",
              "      <td>bereav afghan mother took reveng taliban watch...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>[woman, claim, plastic, surgeri, third, breast]</td>\n",
              "      <td>[bereav, afghan, mother, took, reveng, taliban...</td>\n",
              "      <td>6</td>\n",
              "      <td>180</td>\n",
              "      <td>6</td>\n",
              "      <td>138</td>\n",
              "      <td>[woman_claim, claim_plastic, plastic_surgeri, ...</td>\n",
              "      <td>[bereav_afghan, afghan_mother, mother_took, to...</td>\n",
              "      <td>5</td>\n",
              "      <td>179</td>\n",
              "      <td>5</td>\n",
              "      <td>175</td>\n",
              "      <td>[woman_claim_plastic, claim_plastic_surgeri, p...</td>\n",
              "      <td>[bereav_afghan_mother, afghan_mother_took, mot...</td>\n",
              "      <td>4</td>\n",
              "      <td>178</td>\n",
              "      <td>4</td>\n",
              "      <td>177</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>woman claim plastic surgeri third breast berea...</td>\n",
              "      <td>0.002978</td>\n",
              "      <td>[this, woman, claims, to, have, had, plastic, ...</td>\n",
              "      <td>[a, bereaved, afghan, mother, took, revenge, o...</td>\n",
              "      <td>0.593915</td>\n",
              "      <td>0.019286</td>\n",
              "      <td>[This Woman Claims To Have Had Plastic Surgery...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>[A bereaved Afghan mother took revenge on the ...</td>\n",
              "      <td>0.187857</td>\n",
              "      <td>0.765929</td>\n",
              "      <td>0.046286</td>\n",
              "      <td>-0.408614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Justin Bieber Saved This Guy From a Bear Attack</td>\n",
              "      <td>discuss</td>\n",
              "      <td>When faced with the choice of feasting on a fi...</td>\n",
              "      <td>justin bieber save guy bear attack</td>\n",
              "      <td>face choic feast fine meal human listen justin...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[justin, bieber, save, guy, bear, attack]</td>\n",
              "      <td>[face, choic, feast, fine, meal, human, listen...</td>\n",
              "      <td>6</td>\n",
              "      <td>83</td>\n",
              "      <td>6</td>\n",
              "      <td>76</td>\n",
              "      <td>[justin_bieber, bieber_save, save_guy, guy_bea...</td>\n",
              "      <td>[face_choic, choic_feast, feast_fine, fine_mea...</td>\n",
              "      <td>5</td>\n",
              "      <td>82</td>\n",
              "      <td>5</td>\n",
              "      <td>81</td>\n",
              "      <td>[justin_bieber_save, bieber_save_guy, save_guy...</td>\n",
              "      <td>[face_choic_feast, choic_feast_fine, feast_fin...</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>justin bieber save guy bear attack face choic ...</td>\n",
              "      <td>0.125151</td>\n",
              "      <td>[justin, bieber, saved, this, guy, from, a, be...</td>\n",
              "      <td>[when, faced, with, the, choice, of, feasting,...</td>\n",
              "      <td>0.766177</td>\n",
              "      <td>0.918633</td>\n",
              "      <td>[Justin Bieber Saved This Guy From a Bear Attack]</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.0772</td>\n",
              "      <td>[When faced with the choice of feasting on a f...</td>\n",
              "      <td>0.052167</td>\n",
              "      <td>0.856667</td>\n",
              "      <td>0.091333</td>\n",
              "      <td>0.032367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Argentina president adopts young Jewish boy as...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>ESPN host Chris Fowler was duped by a fake web...</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>espn host chris fowler dupe fake websit mediam...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[argentina, presid, adopt, young, jewish, boy,...</td>\n",
              "      <td>[espn, host, chris, fowler, dupe, fake, websit...</td>\n",
              "      <td>10</td>\n",
              "      <td>265</td>\n",
              "      <td>10</td>\n",
              "      <td>165</td>\n",
              "      <td>[argentina_presid, presid_adopt, adopt_young, ...</td>\n",
              "      <td>[espn_host, host_chris, chris_fowler, fowler_d...</td>\n",
              "      <td>9</td>\n",
              "      <td>264</td>\n",
              "      <td>9</td>\n",
              "      <td>242</td>\n",
              "      <td>[argentina_presid_adopt, presid_adopt_young, a...</td>\n",
              "      <td>[espn_host_chris, host_chris_fowler, chris_fow...</td>\n",
              "      <td>8</td>\n",
              "      <td>263</td>\n",
              "      <td>8</td>\n",
              "      <td>258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>argentina presid adopt young jewish boy godson...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[argentina, president, adopts, young, jewish, ...</td>\n",
              "      <td>[espn, host, chris, fowler, was, duped, by, a,...</td>\n",
              "      <td>0.614044</td>\n",
              "      <td>-0.108503</td>\n",
              "      <td>[Argentina president adopts young Jewish boy a...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.2023</td>\n",
              "      <td>[ESPN host Chris Fowler was duped by a fake we...</td>\n",
              "      <td>0.050389</td>\n",
              "      <td>0.887722</td>\n",
              "      <td>0.061889</td>\n",
              "      <td>0.025100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Iraq claims ISIS leader al-Baghdadi’s wife has...</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>shoot war memori parliament hill unconfirm rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[iraq, claim, isi, leader, al, baghdadi, wife,...</td>\n",
              "      <td>[shoot, war, memori, parliament, hill, unconfi...</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>[iraq_claim, claim_isi, isi_leader, leader_al,...</td>\n",
              "      <td>[shoot_war, war_memori, memori_parliament, par...</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>[iraq_claim_isi, claim_isi_leader, isi_leader_...</td>\n",
              "      <td>[shoot_war_memori, war_memori_parliament, memo...</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>iraq claim isi leader al baghdadi wife arrest ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[iraq, claims, isis, leader, al, baghdadi, s, ...</td>\n",
              "      <td>[there, has, been, a, shooting, at, the, war, ...</td>\n",
              "      <td>0.639913</td>\n",
              "      <td>-0.037467</td>\n",
              "      <td>[Iraq claims ISIS leader al-Baghdadi’s wife ha...</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.807</td>\n",
              "      <td>0.094</td>\n",
              "      <td>-0.0377</td>\n",
              "      <td>[There has been a shooting at the War Memorial...</td>\n",
              "      <td>0.256000</td>\n",
              "      <td>0.744000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.363700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... text_compound\n",
              "0  Sadly, Pumpkin Spice Condoms Aren't A Thing Af...  ...     -0.025750\n",
              "1  This Woman Claims To Have Had Plastic Surgery ...  ...     -0.408614\n",
              "2    Justin Bieber Saved This Guy From a Bear Attack  ...      0.032367\n",
              "3  Argentina president adopts young Jewish boy as...  ...      0.025100\n",
              "4  Iraq claims ISIS leader al-Baghdadi’s wife has...  ...     -0.363700\n",
              "\n",
              "[5 rows x 44 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM1EE-vcPIIm",
        "outputId": "474c20d1-9460-4118-f264-541fce2e7510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# Sentiment Analysis on Binary Class labeled dataset\n",
        "\n",
        "df_final = title_sentiment(df_final)\n",
        "df_final = text_sentiment(df_final)\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>title_unigram</th>\n",
              "      <th>text_unigram</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>title_bigram</th>\n",
              "      <th>text_bigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>title_trigram</th>\n",
              "      <th>text_trigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>cleaned_title_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>word2vec_cleaned_title</th>\n",
              "      <th>word2vec_cleaned_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "      <th>title_sentences</th>\n",
              "      <th>title_neg</th>\n",
              "      <th>title_neu</th>\n",
              "      <th>title_pos</th>\n",
              "      <th>title_compound</th>\n",
              "      <th>text_sentences</th>\n",
              "      <th>text_neg</th>\n",
              "      <th>text_neu</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>text_compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...</td>\n",
              "      <td>Jay Dyer 21st Century WireJamie Hanshaw, autho...</td>\n",
              "      <td>1</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch</td>\n",
              "      <td>jay dyer st centuri wirejami hanshaw author op...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[hollywood, witchcraft, dark, side, reveal, wi...</td>\n",
              "      <td>[jay, dyer, st, centuri, wirejami, hanshaw, au...</td>\n",
              "      <td>6</td>\n",
              "      <td>95</td>\n",
              "      <td>6</td>\n",
              "      <td>74</td>\n",
              "      <td>[hollywood_witchcraft, witchcraft_dark, dark_s...</td>\n",
              "      <td>[jay_dyer, dyer_st, st_centuri, centuri_wireja...</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>91</td>\n",
              "      <td>[hollywood_witchcraft_dark, witchcraft_dark_si...</td>\n",
              "      <td>[jay_dyer_st, dyer_st_centuri, st_centuri_wire...</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>hollywood witchcraft dark side reveal witch ja...</td>\n",
              "      <td>0.079199</td>\n",
              "      <td>[hollywood, witchcraft, the, dark, side, revea...</td>\n",
              "      <td>[jay, dyer, st, century, wirejamie, hanshaw, a...</td>\n",
              "      <td>0.704981</td>\n",
              "      <td>0.792957</td>\n",
              "      <td>[HOLLYWOOD WITCHCRAFT: The Dark Side Revealed ...</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.783</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.3612</td>\n",
              "      <td>[Jay Dyer 21st Century WireJamie Hanshaw, auth...</td>\n",
              "      <td>0.101500</td>\n",
              "      <td>0.874500</td>\n",
              "      <td>0.024000</td>\n",
              "      <td>-0.706000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cyberattack on Belgian media outlets claimed b...</td>\n",
              "      <td>Mon, 24 Oct 2016 21:12 UTC © www,syrian-cyber-...</td>\n",
              "      <td>1</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>mon oct utc www syrian cyber armi net sever be...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[cyberattack, belgian, medium, outlet, claim, ...</td>\n",
              "      <td>[mon, oct, utc, www, syrian, cyber, armi, net,...</td>\n",
              "      <td>8</td>\n",
              "      <td>252</td>\n",
              "      <td>8</td>\n",
              "      <td>180</td>\n",
              "      <td>[cyberattack_belgian, belgian_medium, medium_o...</td>\n",
              "      <td>[mon_oct, oct_utc, utc_www, www_syrian, syrian...</td>\n",
              "      <td>7</td>\n",
              "      <td>251</td>\n",
              "      <td>7</td>\n",
              "      <td>240</td>\n",
              "      <td>[cyberattack_belgian_medium, belgian_medium_ou...</td>\n",
              "      <td>[mon_oct_utc, oct_utc_www, utc_www_syrian, www...</td>\n",
              "      <td>6</td>\n",
              "      <td>250</td>\n",
              "      <td>6</td>\n",
              "      <td>249</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>cyberattack belgian medium outlet claim syrian...</td>\n",
              "      <td>0.231299</td>\n",
              "      <td>[cyberattack, on, belgian, media, outlets, cla...</td>\n",
              "      <td>[mon, oct, utc, www, syrian, cyber, army, net,...</td>\n",
              "      <td>0.708456</td>\n",
              "      <td>0.816659</td>\n",
              "      <td>[Cyberattack on Belgian media outlets claimed ...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>[Mon, 24 Oct 2016 21:12 UTC © www,syrian-cyber...</td>\n",
              "      <td>0.157722</td>\n",
              "      <td>0.794778</td>\n",
              "      <td>0.047500</td>\n",
              "      <td>-0.382922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>France Presidential Election 2017 Livewire</td>\n",
              "      <td>Polls have closed for the second and final rou...</td>\n",
              "      <td>0</td>\n",
              "      <td>franc presidenti elect livewir</td>\n",
              "      <td>poll close second final round vote french pres...</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>[franc, presidenti, elect, livewir]</td>\n",
              "      <td>[poll, close, second, final, round, vote, fren...</td>\n",
              "      <td>4</td>\n",
              "      <td>74</td>\n",
              "      <td>4</td>\n",
              "      <td>65</td>\n",
              "      <td>[franc_presidenti, presidenti_elect, elect_liv...</td>\n",
              "      <td>[poll_close, close_second, second_final, final...</td>\n",
              "      <td>3</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>71</td>\n",
              "      <td>[franc_presidenti_elect, presidenti_elect_live...</td>\n",
              "      <td>[poll_close_second, close_second_final, second...</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>franc presidenti elect livewir poll close seco...</td>\n",
              "      <td>0.022875</td>\n",
              "      <td>[france, presidential, election, livewire]</td>\n",
              "      <td>[polls, have, closed, for, the, second, and, f...</td>\n",
              "      <td>0.471907</td>\n",
              "      <td>0.203756</td>\n",
              "      <td>[France Presidential Election 2017 Livewire]</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>[Polls have closed for the second and final ro...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.929571</td>\n",
              "      <td>0.070429</td>\n",
              "      <td>0.206929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despite Republican’s Best Efforts, SCOTUS Jus...</td>\n",
              "      <td>Much to the dismay of so-called  pro-life  Rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>much dismay call pro life republican suprem co...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[despit, republican, best, effort, scotus, all...</td>\n",
              "      <td>[much, dismay, call, pro, life, republican, su...</td>\n",
              "      <td>10</td>\n",
              "      <td>210</td>\n",
              "      <td>10</td>\n",
              "      <td>133</td>\n",
              "      <td>[despit_republican, republican_best, best_effo...</td>\n",
              "      <td>[much_dismay, dismay_call, call_pro, pro_life,...</td>\n",
              "      <td>9</td>\n",
              "      <td>209</td>\n",
              "      <td>9</td>\n",
              "      <td>194</td>\n",
              "      <td>[despit_republican_best, republican_best_effor...</td>\n",
              "      <td>[much_dismay_call, dismay_call_pro, call_pro_l...</td>\n",
              "      <td>8</td>\n",
              "      <td>208</td>\n",
              "      <td>8</td>\n",
              "      <td>206</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>despit republican best effort scotus allow lou...</td>\n",
              "      <td>0.178999</td>\n",
              "      <td>[despite, republican, s, best, efforts, scotus...</td>\n",
              "      <td>[much, to, the, dismay, of, so, called, pro, l...</td>\n",
              "      <td>0.733165</td>\n",
              "      <td>0.719739</td>\n",
              "      <td>[ Despite Republican’s Best Efforts, SCOTUS Ju...</td>\n",
              "      <td>0.234</td>\n",
              "      <td>0.766</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.5216</td>\n",
              "      <td>[Much to the dismay of so-called  pro-life  Re...</td>\n",
              "      <td>0.073375</td>\n",
              "      <td>0.830375</td>\n",
              "      <td>0.096500</td>\n",
              "      <td>0.149688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNC Chair’s Comment Reveals Why Republicans M...</td>\n",
              "      <td>You almost have to feel sorry for Republicans ...</td>\n",
              "      <td>1</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>almost feel sorri republican almost find reall...</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>[rnc, chair, comment, reveal, republican, may,...</td>\n",
              "      <td>[almost, feel, sorri, republican, almost, find...</td>\n",
              "      <td>9</td>\n",
              "      <td>289</td>\n",
              "      <td>9</td>\n",
              "      <td>210</td>\n",
              "      <td>[rnc_chair, chair_comment, comment_reveal, rev...</td>\n",
              "      <td>[almost_feel, feel_sorri, sorri_republican, re...</td>\n",
              "      <td>8</td>\n",
              "      <td>288</td>\n",
              "      <td>8</td>\n",
              "      <td>282</td>\n",
              "      <td>[rnc_chair_comment, chair_comment_reveal, comm...</td>\n",
              "      <td>[almost_feel_sorri, feel_sorri_republican, sor...</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>rnc chair comment reveal republican may never ...</td>\n",
              "      <td>0.040344</td>\n",
              "      <td>[rnc, chair, s, comment, reveals, why, republi...</td>\n",
              "      <td>[you, almost, have, to, feel, sorry, for, repu...</td>\n",
              "      <td>0.764235</td>\n",
              "      <td>0.472047</td>\n",
              "      <td>[ RNC Chair’s Comment Reveals Why Republicans ...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>[You almost have to feel sorry for Republicans...</td>\n",
              "      <td>0.089375</td>\n",
              "      <td>0.787937</td>\n",
              "      <td>0.122563</td>\n",
              "      <td>0.118056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... text_compound\n",
              "0  HOLLYWOOD WITCHCRAFT: The Dark Side Revealed i...  ...     -0.706000\n",
              "1  Cyberattack on Belgian media outlets claimed b...  ...     -0.382922\n",
              "2         France Presidential Election 2017 Livewire  ...      0.206929\n",
              "3   Despite Republican’s Best Efforts, SCOTUS Jus...  ...      0.149688\n",
              "4   RNC Chair’s Comment Reveals Why Republicans M...  ...      0.118056\n",
              "\n",
              "[5 rows x 44 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0XDZJacHfvO"
      },
      "source": [
        "## Features Distillation for Modelling\n",
        "\n",
        "Now, lets keep the features which are need for model training and remove all the raw title and text features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do4395gUIANt",
        "outputId": "e04c9894-5586-4702-83db-59583a03f931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "distilled_features_cols = ['label', 'count_title_sentences', 'count_text_sentences', 'count_title_unigram', 'count_text_unigram',\n",
        "                'unique_count_title_unigram', 'unique_count_text_unigram', 'count_title_bigram', 'count_text_bigram',\n",
        "                'unique_count_title_bigram', 'unique_count_text_bigram', 'count_title_trigram', 'count_text_trigram',\n",
        "                'unique_count_title_trigram', 'unique_count_text_trigram', 'count_title_unigrams_in_text', 'count_title_bigrams_in_text',\n",
        "                'count_title_trigrams_in_text', 'similarity_title_text', 'topics_similarity_title_text', 'word2vec_similarity_title_text',\n",
        "                'title_neg', 'title_neu', 'title_pos', 'title_compound', 'text_neg', 'text_neu', 'text_pos', 'text_compound']\n",
        "\n",
        "df_final_multi = final_stance[distilled_features_cols]\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df_final_multi['label'])\n",
        "\n",
        "df_final_multi['label'] = le.transform(df_final_multi['label'])\n",
        "# print(le.inverse_transform(df_final_multi['label']))\n",
        "\n",
        "df_final_multi.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>title_neg</th>\n",
              "      <th>title_neu</th>\n",
              "      <th>title_pos</th>\n",
              "      <th>title_compound</th>\n",
              "      <th>text_neg</th>\n",
              "      <th>text_neu</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>text_compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>84</td>\n",
              "      <td>3</td>\n",
              "      <td>92</td>\n",
              "      <td>3</td>\n",
              "      <td>87</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.255239</td>\n",
              "      <td>0.990667</td>\n",
              "      <td>0.779995</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.714</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.4215</td>\n",
              "      <td>0.118500</td>\n",
              "      <td>0.771900</td>\n",
              "      <td>0.109700</td>\n",
              "      <td>-0.025750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>180</td>\n",
              "      <td>6</td>\n",
              "      <td>138</td>\n",
              "      <td>5</td>\n",
              "      <td>179</td>\n",
              "      <td>5</td>\n",
              "      <td>175</td>\n",
              "      <td>4</td>\n",
              "      <td>178</td>\n",
              "      <td>4</td>\n",
              "      <td>177</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002978</td>\n",
              "      <td>0.019286</td>\n",
              "      <td>0.593915</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.187857</td>\n",
              "      <td>0.765929</td>\n",
              "      <td>0.046286</td>\n",
              "      <td>-0.408614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>83</td>\n",
              "      <td>6</td>\n",
              "      <td>76</td>\n",
              "      <td>5</td>\n",
              "      <td>82</td>\n",
              "      <td>5</td>\n",
              "      <td>81</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>4</td>\n",
              "      <td>81</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.125151</td>\n",
              "      <td>0.918633</td>\n",
              "      <td>0.766177</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.0772</td>\n",
              "      <td>0.052167</td>\n",
              "      <td>0.856667</td>\n",
              "      <td>0.091333</td>\n",
              "      <td>0.032367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>265</td>\n",
              "      <td>10</td>\n",
              "      <td>165</td>\n",
              "      <td>9</td>\n",
              "      <td>264</td>\n",
              "      <td>9</td>\n",
              "      <td>242</td>\n",
              "      <td>8</td>\n",
              "      <td>263</td>\n",
              "      <td>8</td>\n",
              "      <td>258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.108503</td>\n",
              "      <td>0.614044</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.2023</td>\n",
              "      <td>0.050389</td>\n",
              "      <td>0.887722</td>\n",
              "      <td>0.061889</td>\n",
              "      <td>0.025100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>10</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.037467</td>\n",
              "      <td>0.639913</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.807</td>\n",
              "      <td>0.094</td>\n",
              "      <td>-0.0377</td>\n",
              "      <td>0.256000</td>\n",
              "      <td>0.744000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.363700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  count_title_sentences  ...  text_pos  text_compound\n",
              "0      0                      1  ...  0.109700      -0.025750\n",
              "1      3                      1  ...  0.046286      -0.408614\n",
              "2      2                      1  ...  0.091333       0.032367\n",
              "3      3                      1  ...  0.061889       0.025100\n",
              "4      3                      1  ...  0.000000      -0.363700\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC_03q-OPWww",
        "outputId": "ad6dff00-16e4-46c8-ed76-4e3aa07c2cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df_final_binary = df_final[distilled_features_cols]\n",
        "df_final_binary.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count_title_sentences</th>\n",
              "      <th>count_text_sentences</th>\n",
              "      <th>count_title_unigram</th>\n",
              "      <th>count_text_unigram</th>\n",
              "      <th>unique_count_title_unigram</th>\n",
              "      <th>unique_count_text_unigram</th>\n",
              "      <th>count_title_bigram</th>\n",
              "      <th>count_text_bigram</th>\n",
              "      <th>unique_count_title_bigram</th>\n",
              "      <th>unique_count_text_bigram</th>\n",
              "      <th>count_title_trigram</th>\n",
              "      <th>count_text_trigram</th>\n",
              "      <th>unique_count_title_trigram</th>\n",
              "      <th>unique_count_text_trigram</th>\n",
              "      <th>count_title_unigrams_in_text</th>\n",
              "      <th>count_title_bigrams_in_text</th>\n",
              "      <th>count_title_trigrams_in_text</th>\n",
              "      <th>similarity_title_text</th>\n",
              "      <th>topics_similarity_title_text</th>\n",
              "      <th>word2vec_similarity_title_text</th>\n",
              "      <th>title_neg</th>\n",
              "      <th>title_neu</th>\n",
              "      <th>title_pos</th>\n",
              "      <th>title_compound</th>\n",
              "      <th>text_neg</th>\n",
              "      <th>text_neu</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>text_compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>95</td>\n",
              "      <td>6</td>\n",
              "      <td>74</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>91</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>4</td>\n",
              "      <td>93</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.079199</td>\n",
              "      <td>0.792957</td>\n",
              "      <td>0.704981</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.783</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.3612</td>\n",
              "      <td>0.101500</td>\n",
              "      <td>0.874500</td>\n",
              "      <td>0.024000</td>\n",
              "      <td>-0.706000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>8</td>\n",
              "      <td>252</td>\n",
              "      <td>8</td>\n",
              "      <td>180</td>\n",
              "      <td>7</td>\n",
              "      <td>251</td>\n",
              "      <td>7</td>\n",
              "      <td>240</td>\n",
              "      <td>6</td>\n",
              "      <td>250</td>\n",
              "      <td>6</td>\n",
              "      <td>249</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.231299</td>\n",
              "      <td>0.816659</td>\n",
              "      <td>0.708456</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.157722</td>\n",
              "      <td>0.794778</td>\n",
              "      <td>0.047500</td>\n",
              "      <td>-0.382922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>74</td>\n",
              "      <td>4</td>\n",
              "      <td>65</td>\n",
              "      <td>3</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>71</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>72</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.022875</td>\n",
              "      <td>0.203756</td>\n",
              "      <td>0.471907</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.929571</td>\n",
              "      <td>0.070429</td>\n",
              "      <td>0.206929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>210</td>\n",
              "      <td>10</td>\n",
              "      <td>133</td>\n",
              "      <td>9</td>\n",
              "      <td>209</td>\n",
              "      <td>9</td>\n",
              "      <td>194</td>\n",
              "      <td>8</td>\n",
              "      <td>208</td>\n",
              "      <td>8</td>\n",
              "      <td>206</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.178999</td>\n",
              "      <td>0.719739</td>\n",
              "      <td>0.733165</td>\n",
              "      <td>0.234</td>\n",
              "      <td>0.766</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.5216</td>\n",
              "      <td>0.073375</td>\n",
              "      <td>0.830375</td>\n",
              "      <td>0.096500</td>\n",
              "      <td>0.149688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>9</td>\n",
              "      <td>289</td>\n",
              "      <td>9</td>\n",
              "      <td>210</td>\n",
              "      <td>8</td>\n",
              "      <td>288</td>\n",
              "      <td>8</td>\n",
              "      <td>282</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>7</td>\n",
              "      <td>287</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.040344</td>\n",
              "      <td>0.472047</td>\n",
              "      <td>0.764235</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.089375</td>\n",
              "      <td>0.787937</td>\n",
              "      <td>0.122563</td>\n",
              "      <td>0.118056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   count_title_sentences  count_text_sentences  ...  text_pos  text_compound\n",
              "0                      1                     2  ...  0.024000      -0.706000\n",
              "1                      1                    18  ...  0.047500      -0.382922\n",
              "2                      1                     7  ...  0.070429       0.206929\n",
              "3                      1                     8  ...  0.096500       0.149688\n",
              "4                      1                    16  ...  0.122563       0.118056\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k4MGqEMdWFp"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hycl1WkYdWl9"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, accuracy_score, \\\n",
        "    classification_report, precision_recall_curve, roc_curve, auc, average_precision_score\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split, cross_val_predict\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxYy8dv1dfaY"
      },
      "source": [
        "multi_classifiers = {\n",
        "    \"LogisticRegression\": (LogisticRegression(max_iter=10000, solver='lbfgs')),\n",
        "    \"KNearest\": (KNeighborsClassifier()),\n",
        "    \"Support Vector Classifier\": (SVC()),\n",
        "    \"DecisionTreeClassifier\": (DecisionTreeClassifier()),\n",
        "    \"Naive Bayes\": (GaussianNB()),\n",
        "    \"Random forests\": (RandomForestClassifier()),\n",
        "    \"XGBoost Classifier\": (XGBClassifier()),\n",
        "    \"GradientBoostingClassifier\": (GradientBoostingClassifier()),\n",
        "    \"MLP Classifier\": (MLPClassifier())\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1UFVBUedmw8"
      },
      "source": [
        "binary_classifiers = {\n",
        "    \"LogisticRegression\": (LogisticRegression(max_iter=10000, solver='lbfgs')),\n",
        "    \"KNearest\": (KNeighborsClassifier()),\n",
        "    \"Support Vector Classifier\": (SVC()),\n",
        "    \"DecisionTreeClassifier\": (DecisionTreeClassifier()),\n",
        "    \"Naive Bayes\": (GaussianNB()),\n",
        "    \"Random forests\": (RandomForestClassifier()),\n",
        "    \"XGBoost Classifier\": (XGBClassifier()),\n",
        "    \"GradientBoostingClassifier\": (GradientBoostingClassifier()),\n",
        "    \"MLP Classifier\": (MLPClassifier())\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KIohUnHdjEL"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range = (0,1))\n",
        "\n",
        "def run_all_models(folds, X_final_train, y_final_train, X_final_test, y_final_test, classifiers):\n",
        "    for classifier_name, classifier in classifiers.items():\n",
        "        k_fold = StratifiedKFold(n_splits=folds, random_state=100, shuffle=True)\n",
        "        cross_val_scores = []\n",
        "        precision_scores = []\n",
        "        recall_scores = []\n",
        "        roc_auc_scores = []\n",
        "        f1_scores = []\n",
        "        for train_index, test_index in k_fold.split(X_final_train, y_final_train):\n",
        "            X_train, X_test = pd.DataFrame(data=X_final_train, index=train_index), pd.DataFrame(data=X_final_train, index=test_index)\n",
        "            y_train, y_test = pd.DataFrame(data=y_final_train, index=train_index), pd.DataFrame(data=y_final_train, index=test_index)\n",
        "            model = classifier\n",
        "\n",
        "            model.fit(X_train, y_train.values.ravel())\n",
        "            scores = cross_val_score(model, X_train, y_train.values.ravel(), cv=5)\n",
        "            cross_val_scores.append(scores)\n",
        "            y_pred = model.predict(X_test)\n",
        "            precision_scores.append(precision_score(y_test, y_pred, average=None))\n",
        "            recall_scores.append(recall_score(y_test, y_pred, average=None))\n",
        "            # roc_auc_scores.append(roc_auc_score(y_test, model.predict_proba(X_test), average='macro', multi_class='ovo'))\n",
        "            f1_scores.append(f1_score(y_test, y_pred, average=None))\n",
        "\n",
        "        print('============================= {} ============================='.format(classifier_name))\n",
        "        print('Mean cross validation score: {}'.format(np.array([cross_val_scores]).mean()))\n",
        "        print('Mean precision score: {}'.format(np.array([precision_scores]).mean()))\n",
        "        print('Mean Recall score: {}'.format(np.array([recall_scores]).mean()))\n",
        "        print('Mean ROC-AUC score: {}'.format(np.array([roc_auc_scores]).mean()))\n",
        "        print('Mean F1 score: {}'.format(np.array([f1_scores]).mean()))\n",
        "        print('******* Real test dataset metrics *******')\n",
        "        y_final_pred = model.predict((X_final_test))\n",
        "        print('Accuracy score for the real test set:\\n', accuracy_score(y_final_test, y_final_pred))\n",
        "        print('confusion matrix for the real test set:\\n', confusion_matrix(y_final_test, y_final_pred))\n",
        "        print('Classification report for the real test set:\\n', classification_report(y_final_test, y_final_pred))\n",
        "        # if classifier_name != 'Support Vector Classifier':\n",
        "        #     y_final_pred_prob = model.predict_proba(X_test)\n",
        "        #     plot_auc_roc_curve(y_test, y_final_pred_prob[:, 1], classifier_name)\n",
        "        #     plot_precision_recall_curve(y_test, y_final_pred_prob[:, 1], classifier_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgJue4skdoBR"
      },
      "source": [
        "## AUC-ROC Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXSUFF29dlEH"
      },
      "source": [
        "def plot_auc_roc_curve(y_test, y_pred, name):\n",
        "    fpr, tpr, thresholds = roc_curve(y_test.to_numpy(), y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC\n",
        "    plt.title('ROC for {}'.format(name))\n",
        "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.plot([0, 1], [0, 1], 'r--')\n",
        "    plt.xlim([-0.1, 1.0])\n",
        "    plt.ylim([-0.1, 1.01])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW4oNmwhdsyN"
      },
      "source": [
        "## Precision Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fNcPlK5dnEE"
      },
      "source": [
        "def plot_precision_recall_curve(y_test, y_pred_prob, name):\n",
        "    # Generate precision recall curve values: precision, recall, thresholds\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test.to_numpy(), y_pred_prob)\n",
        "\n",
        "    # Plot Precision Recall curve\n",
        "    plt.plot(recall, precision)\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    avg_precision_score = average_precision_score(y_test, y_pred_prob)\n",
        "    plt.title('PRC for {} - avg precision score: {}'.format(name, str(avg_precision_score)))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urVe9bQVXuF4"
      },
      "source": [
        "### Model training on Multiclass classification dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH7uSW1Hdxio",
        "outputId": "af4c60a1-1480-4f9c-a33a-0b14107a05e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y = df_final_multi['label']\n",
        "X = df_final_multi.drop(['label'], axis=1)\n",
        "\n",
        "X.reset_index(drop=True, inplace=True)\n",
        "y.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X_final_train, X_final_test, y_final_train, y_final_test = train_test_split(X, y, test_size=0.20, random_state=100, stratify=y)\n",
        "X_final_train.reset_index(drop=True, inplace=True)\n",
        "X_final_test.reset_index(drop=True, inplace=True)\n",
        "y_final_train.reset_index(drop=True, inplace=True)\n",
        "y_final_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "run_all_models(5, X_final_train, y_final_train, X_final_test, y_final_test, multi_classifiers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================= LogisticRegression =============================\n",
            "Mean cross validation score: 0.8652577319587629\n",
            "Mean precision score: 0.562245942725083\n",
            "Mean Recall score: 0.5282183908045976\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.521823291084269\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.8675496688741722\n",
            "confusion matrix for the real test set:\n",
            " [[  3   0   8   1]\n",
            " [  1   0   2   0]\n",
            " [  5   0  19   3]\n",
            " [  0   0   0 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.25      0.29        12\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.66      0.70      0.68        27\n",
            "           3       0.96      1.00      0.98       109\n",
            "\n",
            "    accuracy                           0.87       151\n",
            "   macro avg       0.49      0.49      0.49       151\n",
            "weighted avg       0.84      0.87      0.85       151\n",
            "\n",
            "============================= KNearest =============================\n",
            "Mean cross validation score: 0.6820146048109966\n",
            "Mean precision score: 0.25231043543543547\n",
            "Mean Recall score: 0.25332978802806383\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.24009423824746406\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.6821192052980133\n",
            "confusion matrix for the real test set:\n",
            " [[  0   0   0  12]\n",
            " [  0   0   1   2]\n",
            " [  3   0   1  23]\n",
            " [  3   0   4 102]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        12\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.17      0.04      0.06        27\n",
            "           3       0.73      0.94      0.82       109\n",
            "\n",
            "    accuracy                           0.68       151\n",
            "   macro avg       0.23      0.24      0.22       151\n",
            "weighted avg       0.56      0.68      0.60       151\n",
            "\n",
            "============================= Support Vector Classifier =============================\n",
            "Mean cross validation score: 0.7255412371134021\n",
            "Mean precision score: 0.2813686714355163\n",
            "Mean Recall score: 0.25476190476190474\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.21931407604895753\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.7218543046357616\n",
            "confusion matrix for the real test set:\n",
            " [[  0   0   0  12]\n",
            " [  0   0   0   3]\n",
            " [  0   0   0  27]\n",
            " [  0   0   0 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        12\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.00      0.00      0.00        27\n",
            "           3       0.72      1.00      0.84       109\n",
            "\n",
            "    accuracy                           0.72       151\n",
            "   macro avg       0.18      0.25      0.21       151\n",
            "weighted avg       0.52      0.72      0.61       151\n",
            "\n",
            "============================= DecisionTreeClassifier =============================\n",
            "Mean cross validation score: 0.8429209621993127\n",
            "Mean precision score: 0.5151608085964069\n",
            "Mean Recall score: 0.5041547867840972\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.5064073034365377\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.8543046357615894\n",
            "confusion matrix for the real test set:\n",
            " [[  3   0   9   0]\n",
            " [  2   0   1   0]\n",
            " [  7   2  17   1]\n",
            " [  0   0   0 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.25      0.25        12\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.63      0.63      0.63        27\n",
            "           3       0.99      1.00      1.00       109\n",
            "\n",
            "    accuracy                           0.85       151\n",
            "   macro avg       0.47      0.47      0.47       151\n",
            "weighted avg       0.85      0.85      0.85       151\n",
            "\n",
            "============================= Naive Bayes =============================\n",
            "Mean cross validation score: 0.7752835051546392\n",
            "Mean precision score: 0.4898021185105019\n",
            "Mean Recall score: 0.49327418271383794\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.4263295797461953\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.7814569536423841\n",
            "confusion matrix for the real test set:\n",
            " [[  4   8   0   0]\n",
            " [  1   2   0   0]\n",
            " [  3  16   6   2]\n",
            " [  0   0   3 106]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.33      0.40        12\n",
            "           1       0.08      0.67      0.14         3\n",
            "           2       0.67      0.22      0.33        27\n",
            "           3       0.98      0.97      0.98       109\n",
            "\n",
            "    accuracy                           0.78       151\n",
            "   macro avg       0.56      0.55      0.46       151\n",
            "weighted avg       0.87      0.78      0.80       151\n",
            "\n",
            "============================= Random forests =============================\n",
            "Mean cross validation score: 0.8773496563573883\n",
            "Mean precision score: 0.5257054119413669\n",
            "Mean Recall score: 0.5201423097974821\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.5125382794215548\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.8874172185430463\n",
            "confusion matrix for the real test set:\n",
            " [[  5   0   6   1]\n",
            " [  1   0   2   0]\n",
            " [  5   0  20   2]\n",
            " [  0   0   0 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.42      0.43        12\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.71      0.74      0.73        27\n",
            "           3       0.97      1.00      0.99       109\n",
            "\n",
            "    accuracy                           0.89       151\n",
            "   macro avg       0.54      0.54      0.54       151\n",
            "weighted avg       0.87      0.89      0.88       151\n",
            "\n",
            "============================= XGBoost Classifier =============================\n",
            "Mean cross validation score: 0.8656958762886597\n",
            "Mean precision score: 0.47590654915444536\n",
            "Mean Recall score: 0.48549472558093243\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.47527095294851057\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.8874172185430463\n",
            "confusion matrix for the real test set:\n",
            " [[  3   0   9   0]\n",
            " [  1   0   2   0]\n",
            " [  3   0  22   2]\n",
            " [  0   0   0 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.25      0.32        12\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.67      0.81      0.73        27\n",
            "           3       0.98      1.00      0.99       109\n",
            "\n",
            "    accuracy                           0.89       151\n",
            "   macro avg       0.52      0.52      0.51       151\n",
            "weighted avg       0.86      0.89      0.87       151\n",
            "\n",
            "============================= GradientBoostingClassifier =============================\n",
            "Mean cross validation score: 0.8665420962199313\n",
            "Mean precision score: 0.5248026056458734\n",
            "Mean Recall score: 0.5131866074538489\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.5086824188988682\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.8874172185430463\n",
            "confusion matrix for the real test set:\n",
            " [[  5   1   6   0]\n",
            " [  1   0   2   0]\n",
            " [  6   0  20   1]\n",
            " [  0   0   0 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.42      0.42        12\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.71      0.74      0.73        27\n",
            "           3       0.99      1.00      1.00       109\n",
            "\n",
            "    accuracy                           0.89       151\n",
            "   macro avg       0.53      0.54      0.53       151\n",
            "weighted avg       0.88      0.89      0.88       151\n",
            "\n",
            "============================= MLP Classifier =============================\n",
            "Mean cross validation score: 0.8328994845360826\n",
            "Mean precision score: 0.48338885308736773\n",
            "Mean Recall score: 0.473323692590934\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.4438307899249419\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.7814569536423841\n",
            "confusion matrix for the real test set:\n",
            " [[  7   0   0   5]\n",
            " [  3   0   0   0]\n",
            " [ 16   0   2   9]\n",
            " [  0   0   0 109]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.58      0.37        12\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       1.00      0.07      0.14        27\n",
            "           3       0.89      1.00      0.94       109\n",
            "\n",
            "    accuracy                           0.78       151\n",
            "   macro avg       0.54      0.41      0.36       151\n",
            "weighted avg       0.84      0.78      0.73       151\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWWSRTASicfB"
      },
      "source": [
        "### Multiclass Classification Results\n",
        "\n",
        "As we can see above, Random Forest and XgBoost models perfomed well for multi classfication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmJz9CgjYD63"
      },
      "source": [
        "### Model training on Binary classification dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otk2YiH1ip1k"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_H42gWpYD65",
        "outputId": "b48cee54-c793-4342-cb6e-efb044773381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_final_binary.fillna(0, inplace=True)\n",
        "\n",
        "y = df_final_binary['label']\n",
        "X = df_final_binary.drop(['label'], axis=1)\n",
        "\n",
        "X.reset_index(drop=True, inplace=True)\n",
        "y.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X_final_train, X_final_test, y_final_train, y_final_test = train_test_split(X, y, test_size=0.20, random_state=100, stratify=y)\n",
        "X_final_train.reset_index(drop=True, inplace=True)\n",
        "X_final_test.reset_index(drop=True, inplace=True)\n",
        "y_final_train.reset_index(drop=True, inplace=True)\n",
        "y_final_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "run_all_models(5, X_final_train, y_final_train, X_final_test, y_final_test, binary_classifiers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================= LogisticRegression =============================\n",
            "Mean cross validation score: 0.6825592185592185\n",
            "Mean precision score: 0.6873590009312579\n",
            "Mean Recall score: 0.6883311948435289\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.687230704682151\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.7464788732394366\n",
            "confusion matrix for the real test set:\n",
            " [[44 19]\n",
            " [17 62]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.70      0.71        63\n",
            "           1       0.77      0.78      0.77        79\n",
            "\n",
            "    accuracy                           0.75       142\n",
            "   macro avg       0.74      0.74      0.74       142\n",
            "weighted avg       0.75      0.75      0.75       142\n",
            "\n",
            "============================= KNearest =============================\n",
            "Mean cross validation score: 0.5859536019536019\n",
            "Mean precision score: 0.6057694163221461\n",
            "Mean Recall score: 0.6052796602511972\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.5975643758378772\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.5633802816901409\n",
            "confusion matrix for the real test set:\n",
            " [[35 28]\n",
            " [34 45]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.56      0.53        63\n",
            "           1       0.62      0.57      0.59        79\n",
            "\n",
            "    accuracy                           0.56       142\n",
            "   macro avg       0.56      0.56      0.56       142\n",
            "weighted avg       0.57      0.56      0.56       142\n",
            "\n",
            "============================= Support Vector Classifier =============================\n",
            "Mean cross validation score: 0.5735824175824176\n",
            "Mean precision score: 0.5728671135073959\n",
            "Mean Recall score: 0.5435363995060389\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.5085551710642744\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.6056338028169014\n",
            "confusion matrix for the real test set:\n",
            " [[12 51]\n",
            " [ 5 74]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.19      0.30        63\n",
            "           1       0.59      0.94      0.73        79\n",
            "\n",
            "    accuracy                           0.61       142\n",
            "   macro avg       0.65      0.56      0.51       142\n",
            "weighted avg       0.64      0.61      0.54       142\n",
            "\n",
            "============================= DecisionTreeClassifier =============================\n",
            "Mean cross validation score: 0.6648498168498169\n",
            "Mean precision score: 0.654339879803891\n",
            "Mean Recall score: 0.6544614318845817\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.653040581373918\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.6056338028169014\n",
            "confusion matrix for the real test set:\n",
            " [[38 25]\n",
            " [31 48]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.60      0.58        63\n",
            "           1       0.66      0.61      0.63        79\n",
            "\n",
            "    accuracy                           0.61       142\n",
            "   macro avg       0.60      0.61      0.60       142\n",
            "weighted avg       0.61      0.61      0.61       142\n",
            "\n",
            "============================= Naive Bayes =============================\n",
            "Mean cross validation score: 0.7077216117216117\n",
            "Mean precision score: 0.7362248502981046\n",
            "Mean Recall score: 0.7358472937562122\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.7294325862367975\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.6126760563380281\n",
            "confusion matrix for the real test set:\n",
            " [[39 24]\n",
            " [31 48]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.62      0.59        63\n",
            "           1       0.67      0.61      0.64        79\n",
            "\n",
            "    accuracy                           0.61       142\n",
            "   macro avg       0.61      0.61      0.61       142\n",
            "weighted avg       0.62      0.61      0.61       142\n",
            "\n",
            "============================= Random forests =============================\n",
            "Mean cross validation score: 0.7574358974358973\n",
            "Mean precision score: 0.7575401745948714\n",
            "Mean Recall score: 0.7577498870515949\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.7561320288936606\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.676056338028169\n",
            "confusion matrix for the real test set:\n",
            " [[41 22]\n",
            " [24 55]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.65      0.64        63\n",
            "           1       0.71      0.70      0.71        79\n",
            "\n",
            "    accuracy                           0.68       142\n",
            "   macro avg       0.67      0.67      0.67       142\n",
            "weighted avg       0.68      0.68      0.68       142\n",
            "\n",
            "============================= XGBoost Classifier =============================\n",
            "Mean cross validation score: 0.7504273504273504\n",
            "Mean precision score: 0.7558756717858364\n",
            "Mean Recall score: 0.7550285232372518\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.7539560036024355\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.7253521126760564\n",
            "confusion matrix for the real test set:\n",
            " [[43 20]\n",
            " [19 60]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.68      0.69        63\n",
            "           1       0.75      0.76      0.75        79\n",
            "\n",
            "    accuracy                           0.73       142\n",
            "   macro avg       0.72      0.72      0.72       142\n",
            "weighted avg       0.72      0.73      0.73       142\n",
            "\n",
            "============================= GradientBoostingClassifier =============================\n",
            "Mean cross validation score: 0.7464322344322344\n",
            "Mean precision score: 0.7600636785627126\n",
            "Mean Recall score: 0.758243667359417\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.7584669632225245\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.676056338028169\n",
            "confusion matrix for the real test set:\n",
            " [[37 26]\n",
            " [20 59]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.59      0.62        63\n",
            "           1       0.69      0.75      0.72        79\n",
            "\n",
            "    accuracy                           0.68       142\n",
            "   macro avg       0.67      0.67      0.67       142\n",
            "weighted avg       0.67      0.68      0.67       142\n",
            "\n",
            "============================= MLP Classifier =============================\n",
            "Mean cross validation score: 0.6282637362637363\n",
            "Mean precision score: 0.5341776737286447\n",
            "Mean Recall score: 0.5539128339507846\n",
            "Mean ROC-AUC score: nan\n",
            "Mean F1 score: 0.503766318145259\n",
            "******* Real test dataset metrics *******\n",
            "Accuracy score for the real test set:\n",
            " 0.647887323943662\n",
            "confusion matrix for the real test set:\n",
            " [[18 45]\n",
            " [ 5 74]]\n",
            "Classification report for the real test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.29      0.42        63\n",
            "           1       0.62      0.94      0.75        79\n",
            "\n",
            "    accuracy                           0.65       142\n",
            "   macro avg       0.70      0.61      0.58       142\n",
            "weighted avg       0.69      0.65      0.60       142\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl14ZRgYi5kU"
      },
      "source": [
        "### Binary Classification Results\n",
        "\n",
        "As we can see above, Gradient Boost Classifierperfomed well for binary classfication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v65xaqcWv46T"
      },
      "source": [
        "## Prediction and Testing\n",
        "Lets create methods that take title and text, generate all features required for the model and predict the multi class label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uSfNzhxwMAN"
      },
      "source": [
        "def get_distilled_dataset(title, text):\n",
        "  data = {'title': [title], 'text': [text]}\n",
        "  df_test = pd.DataFrame(data)\n",
        "\n",
        "\t# Add the cleaned columns\n",
        "  df_test['cleaned_title'] = df_test[\"title\"].map(lambda x: cleaning(x))\n",
        "  df_test['cleaned_text'] = df_test[\"text\"].map(lambda x: cleaning(x))\n",
        "\n",
        "\t# Add sentence count columns\n",
        "  count_sentences(df_test)\n",
        "\n",
        "\t# Add ngram features\n",
        "  generate_ngram_features(df_test)\n",
        "\n",
        "\t# Add common ngram counts\n",
        "  common_ngrams_in_text(df_test)\n",
        "\n",
        "\t# Add TF_IDF and similarity scores\n",
        "  title_tfidf_vectors, text_tfidf_vectors = tf_idf_similarity_score(df_test)\n",
        "\n",
        "\t# Add word2vec similarity scores\n",
        "  word2vec_similarity_score(df_test)\n",
        "\n",
        "\t# Add Topic Similarity score\n",
        "  topic_similarity_score(df_test, title_tfidf_vectors, text_tfidf_vectors)\n",
        "\n",
        "\t# Add Sentiment scores\n",
        "  df_test = title_sentiment(df_test)\n",
        "  df_test = text_sentiment(df_test)\n",
        "\n",
        "\t# Distill features\n",
        "  X_cols = [x for i,x in enumerate(distilled_features_cols) if x!='label']\n",
        "  return df_test[X_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztX7Sc4dbhx5"
      },
      "source": [
        "## Pickle the models\n",
        "\n",
        "Pickling the trained models so that they can be loaded into the the notebook that has other feature analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rAzNiSgbhGk"
      },
      "source": [
        "import pickle\n",
        "\n",
        "multi_class_model = multi_classifiers['Random forests']\n",
        "binary_class_model = binary_classifiers['GradientBoostingClassifier']\n",
        "\n",
        "\n",
        "multi_class_model_path = '/content/drive/Shared drives/SheCodes/MLSpring2020/shecodes_employee_attrition/Alternus Vera Sprint 4/Models/multi_class_model.pickle'\n",
        "binary_class_model_path = '/content/drive/Shared drives/SheCodes/MLSpring2020/shecodes_employee_attrition/Alternus Vera Sprint 4/Models/binary_class_model.pickle'\n",
        "\n",
        "\n",
        "pickle.dump(multi_class_model, open(multi_class_model_path, 'wb'))\n",
        "pickle.dump(binary_class_model, open(binary_class_model_path, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8NC6gTEf4MT"
      },
      "source": [
        "multi_class_model = pickle.load(open(multi_class_model_path, \"rb\"))\n",
        "binary_class_model = pickle.load(open(binary_class_model_path, \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmjRwnG8G3cK"
      },
      "source": [
        "def predict_multiclass(title, text):\n",
        "  df_test = get_distilled_dataset(title, text)\n",
        "  return multi_classifiers['Random forests'].predict(df_test), multi_classifiers['Random forests'].predict_proba(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF0P_BZmG9Wd"
      },
      "source": [
        "def predict_binaryclass(title, text):\n",
        "  df_test = get_distilled_dataset(title, text)\n",
        "  return binary_classifiers['GradientBoostingClassifier'].predict(df_test), binary_classifiers['GradientBoostingClassifier'].predict_proba(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqqthy1rIfxq"
      },
      "source": [
        "My Polynomial equation gives 60%, 40% weights to the accuracies respectively given by multi label classification predicted by Random Forest and binary classification predicted by Gradient Boost Classifer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRFc7Ma6JV-o"
      },
      "source": [
        "For generating the fakeness score in the range of 0 to 1, where 1 denotes the news is absolutely fake, I am considering `Agrees` and `Discusses` categories from the Fake News Dataset as True classification. Remaining labels `Disagrees` and `Unrelated` are considered fake. \n",
        "\n",
        "If prediction agrees, probability of being True is higher as many features might have correlated. So I am giving weight of 40% to `Agrees`, 20% to `Discusses`, 30% to `Disagrees` and 10% to `Unrelated`.\n",
        "\n",
        "So My polynomial would be something like below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZoUIcqgOB3d"
      },
      "source": [
        "def Shecodes_getTitleVsBodyScore(title, text):\n",
        "  prediction_multi, prediction_multi_prob = predict_multiclass(title, text)\n",
        "  print (prediction_multi)\n",
        "  print (\"Multi label predicted classification: {}\".format(le.inverse_transform(prediction_multi)[0]))\n",
        "  print (\"Multi label prediction probabilities {} : {}\".format(le.inverse_transform([0,1,2,3]), prediction_multi_prob))\n",
        "\n",
        "  prediction_binary, prediction_binary_prob = predict_binaryclass(title, text)\n",
        "  print (prediction_binary)\n",
        "  print (\"Binary predicted classification: {}\".format(\"Fake\" if prediction_binary[0] == 1 else \"True\"))\n",
        "  print (\"Binary label prediction probabilities ['True', 'Fake']: {}\".format(prediction_binary_prob))\n",
        "  print (prediction_multi)\n",
        "\n",
        "  multiclass_truth_score = (prediction_multi_prob[0][0] * 0.6 + prediction_multi_prob[0][2] + 0.4)\n",
        "  multiclass_fake_score = (prediction_multi_prob[0][1] * 0.6 + prediction_multi_prob[0][3] + 0.4)\n",
        "\n",
        "  binaryclass_truth_score = (prediction_binary_prob[0][0] * 0.5)\n",
        "  binaryclass_fake_score = (prediction_binary_prob[0][1] * 0.5)\n",
        "\n",
        "  overall_truth_score = (0.6 * multiclass_truth_score) + (0.4 * binaryclass_truth_score)\n",
        "  overall_fakeness_score = (0.6 * multiclass_fake_score) + (0.4 * binaryclass_fake_score)\n",
        "\n",
        "  if (overall_truth_score > overall_fakeness_score):\n",
        "    if (overall_truth_score > 1):\n",
        "      overall_truth_score = 1\n",
        "    print (\"Overall Fakeness Score: {}\", 1 - overall_truth_score)\n",
        "    return 1 - overall_truth_score\n",
        "  else:\n",
        "    if (overall_fakeness_score > 1):\n",
        "      overall_fakeness_score = 1\n",
        "    print (\"Overall Fakeness Score: {}\", overall_fakeness_score)\n",
        "    return overall_fakeness_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGjX-t-MH2Oz",
        "outputId": "9272feef-1360-4ed1-9e76-6cee414aa9af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "score = Shecodes_getTitleVsBodyScore (\n",
        "  \"A Russian Guy Says His Justin Bieber Ringtone Saved Him From A Bear Attack\", \n",
        "  \"A bereaved Afghan mother took revenge on the Taliban after watching them kill her son in an ambush. Reza Gul killed 25 Taliban fighters and injured five others in a seven-hour gunbattle in Farah province. Gul, who was joined by her daughter and daughter in-law, engaged the Taliban using AK-47s and grenades, despitenever before having used a weapon. The embattled mother told Tolo news, a 24-hour Afghan news broadcaster, she was awakened by shots early Tuesday. After seeing that her son had been killed, Gul and the other two women fought back. “I couldn't stop myself and picked up a weapon,” Gul told Tolo News. “I went to the check post and began shooting back.” Seema, her daughter-in-law, added: “The fighting was intensified when we reached the battlefield along with light and heavy weapons. We were committed to fight until the last bullet.” Gul said that the battlefield was covered in Talib fighters after the deadly exchange ended. While the Taliban have not publicly commented on the incident, the Afghan government labeled it a symbol of a public uprising against the Taliban. Taliban and other groups have regained large swathes of the country as U.S. and NATO forces slowly pull out troops after 14 years of war. The Taliban have targeted government and foreign infrastructure as the group attempts to claw back power it lost in 2001. While the Taliban have made key gains in rural regions, members continue to employ suicide bomber tactics in well protected towns and cities. Earlier this week, 50 people were killed after a suicide bomber detonated a vest during a volleyball competition in Yahyakahil, Paktika province. That particular attack prompted President Ashraf Ghani to order a complete overview of the country’s defense forces and to rethink the ban on nighttime raids, which were outlawed by his predecessor, Hamid Karzai.\"\n",
        ")\n",
        "print (score)\n",
        "\n",
        "score = Shecodes_getTitleVsBodyScore (\n",
        "  \"Donald Trump Sends Out Embarrassing New Year‚Äôs Eve Message; This is Disturbing\", \n",
        "  \"Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.\"\n",
        ")\n",
        "print (score)\n",
        "\n",
        "score = Shecodes_getTitleVsBodyScore (\n",
        "  \"Bad News For Trump ‚Äî Mitch McConnell Says No To Repealing Obamacare In 2018\", \n",
        "  \"Republicans have had seven years to come up with a viable replacement for Obamacare but they failed miserably. After taking a victory lap for gifting the wealthy with a tax break on Wednesday, Donald Trump looked at the cameras and said,  We have essentially repealed Obamacare and we will come up with something that will be much better. Obamacare has been repealed in this bill,  he added. Well, like most things Trump says, that s just not true. But, if the former reality show star could have done that in order to eradicate former President Obama s signature legislation, he would have and without offering an alternative.Senate Majority Leader Mitch McConnell told NPR that  This has not been a very bipartisan year. I hope in the new year, we re going to pivot here and become more cooperative. An Obamacare repeal in 2018 is DOA. Well, we obviously were unable to completely repeal and replace with a 52-48 Senate,  the Kentucky Republican said.  We ll have to take a look at what that looks like with a 51-49 Senate. But I think we ll probably move on to other issues. NPR reports:McConnell hopes to focus instead on stabilizing the insurance marketplaces to keep premiums from skyrocketing in the early months of 2018, a promise he made to moderate Republican Sen. Susan Collins of Maine to get her support for the tax bill.On top of that McConnell broke with House Speaker Paul Ryan, R-Wis., on the approach to paring back spending on programs like Medicaid and food stamps. McConnell told NPR he is  not interested  in using Senate budget rules to allow Republicans to cut entitlements without consultation with Democrats. I think entitlement changes, to be sustained, almost always have to be bipartisan,  McConnell said.  The House may have a different agenda. If our Democratic friends in the Senate want to join us to tackle any kind of entitlement reform. I d be happy to take a look at it. This is coming from Mitch McConnell. He knows Donald Trump is destroying the GOP. It doesn t matter, Sen. McConnell. We still recall him saying that his  number one priority is making sure president Obama s a one-term president. Well, we re hoping that Trump doesn t last a full term. Funny how that works.Photo by Chip Somodevilla/Getty Images\"\n",
        ")\n",
        "print (score)\n",
        "\n",
        "score = Shecodes_getTitleVsBodyScore (\n",
        "  \" Judge Who Barred A Mom From Seeing Her Baby For A Year Over Unpaid Fees Resigns\", \n",
        "  \"With all the recent talk of sentencing reform and unfair bail across America, sometimes what it takes is an outrageous case in a small town somewhere to light a fire under activists to demand a change. And even though the resignation of a judge in Pearl, Mississippi, a suburb of the capital with a meager 25,000 residents, isn t really anyone s idea of a revolution, it probably means a lot to the mother at the center of the case.Judge John Shirley of the now-dissolved Pearl Youth Court decided back in August of 2016 that some unpaid court fees were enough to separate  Mother A,  a resident of nearby Jackson, from her baby for a period of 14 months. The unidentified mom and a friend were driving through Pearl one day looking for work when they were pulled over by a police officer who discovered that both women had misdemeanor warrants for minor offenses.Honestly, the story begins with that officer, who is also unidentified in the report from the Clarion-Ledger, the news outlet that first reported the judge s resignation. So much begins with a cop s interaction with a young person   will they detain them? Will they give them a warning? Initial contact with law enforcement often determines the course of a young person s life, and most police are totally aware of that fact. Mother A  had her baby with her that day, and although the woman s grandmother came to the scene immediately to collect the baby, that officer decided to change the young woman s life by requiring her to bring the child before the Pearl Youth Court.If it seems like I m being too hard on the cop, let me assure you, I ve saved most of my hate for Judge Shirley. How an arbiter of justice thinks ordering a young mother to stay away from her newborn baby is anything approaching justice is bewildering to me. But that s just what Judge Shirley did, remanding the infant to the custody of the grandmother and imposing a goddamn NO CONTACT ORDER between Mother A and her baby until the poor (as in, without money) young woman could pay off her court fees.Not only could she not see her child, she couldn t see her grandmother while the baby was present. It was just to be cruel. That s the only explanation.Cliff Johnson, the Director of the Roderick and Solange MacArthur Justice Center at the University of Mississippi School of Law, agreed. As a civil rights lawyer in Mississippi, I am no stranger to injustice, but for a judge to prohibit an impoverished mother from having any contact with her baby until monetary payments are made is shocking and repugnant. Such orders are tantamount to judicial kidnapping. Between two people   one cop and one judge   a young woman s life is forever changed, her idea of justice forever skewed, her trust in authority forever hobbled by the fear of a cruelty that exists only for its own sake.Featured image via judgejohnshirley.com\"\n",
        ")\n",
        "print (score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 513)\n",
            "(1, 513)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[3]\n",
            "Multi label predicted classification: unrelated\n",
            "Multi label prediction probabilities ['agree' 'disagree' 'discuss' 'unrelated'] : [[0.01 0.   0.   0.99]]\n",
            "(1, 513)\n",
            "(1, 513)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[1]\n",
            "Binary predicted classification: Fake\n",
            "Binary label prediction probabilities ['True', 'Fake']: [[0.06893351 0.93106649]]\n",
            "[3]\n",
            "Overall Fakeness Score: {} 1\n",
            "1\n",
            "(1, 617)\n",
            "(1, 617)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[2]\n",
            "Multi label predicted classification: discuss\n",
            "Multi label prediction probabilities ['agree' 'disagree' 'discuss' 'unrelated'] : [[0.18 0.09 0.6  0.13]]\n",
            "(1, 617)\n",
            "(1, 617)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[0]\n",
            "Binary predicted classification: True\n",
            "Binary label prediction probabilities ['True', 'Fake']: [[0.56387717 0.43612283]]\n",
            "[2]\n",
            "Overall Fakeness Score: {} 0.2224245667599537\n",
            "0.2224245667599537\n",
            "(1, 585)\n",
            "(1, 585)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[2]\n",
            "Multi label predicted classification: discuss\n",
            "Multi label prediction probabilities ['agree' 'disagree' 'discuss' 'unrelated'] : [[0.25 0.04 0.46 0.25]]\n",
            "(1, 585)\n",
            "(1, 585)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[1]\n",
            "Binary predicted classification: Fake\n",
            "Binary label prediction probabilities ['True', 'Fake']: [[0.4733929 0.5266071]]\n",
            "[2]\n",
            "Overall Fakeness Score: {} 0.29932142034039777\n",
            "0.29932142034039777\n",
            "(1, 702)\n",
            "(1, 702)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[2]\n",
            "Multi label predicted classification: discuss\n",
            "Multi label prediction probabilities ['agree' 'disagree' 'discuss' 'unrelated'] : [[0.22 0.02 0.51 0.25]]\n",
            "(1, 702)\n",
            "(1, 702)\n",
            "(1, 2)\n",
            "(1, 2)\n",
            "[0]\n",
            "Binary predicted classification: True\n",
            "Binary label prediction probabilities ['True', 'Fake']: [[0.64332162 0.35667838]]\n",
            "[2]\n",
            "Overall Fakeness Score: {} 0.2461356761225708\n",
            "0.2461356761225708\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}